{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaper   : https://arxiv.org/pdf/1808.00508v1.pdf\\nAuthors : Andrew Trask, Felix Hill, Scott Reed, Jack Rae\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paper   : https://arxiv.org/pdf/1808.00508v1.pdf\n",
    "Authors : Andrew Trask, Felix Hill, Scott Reed, Jack Rae\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Accumulator\n",
    "\n",
    "![Neural Accumulator](https://cdn-images-1.medium.com/max/1600/1*vMYerlUvUP5gw4LDZv-aSg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumulator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NeuralAccumulator, self).__init__()\n",
    "        self.W1 = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W2 = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.register_parameter('bias', None)\n",
    "        \n",
    "        self.W = Parameter(torch.tanh(self.W1) * torch.sigmoid(self.W2))\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = nn.functional.linear(x, self.W, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Accumulator for Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(train_size, test_size, fn):\n",
    "    X = torch.Tensor(train_size + test_size, 2)\n",
    "    Y = torch.Tensor(train_size + test_size, 1)\n",
    "\n",
    "    for i in range(train_size + test_size):\n",
    "        x = torch.rand(2)*10\n",
    "        y = torch.tensor([fn(*x)])\n",
    "        X[i] = x\n",
    "        Y[i] = y\n",
    "    \n",
    "    X_train, y_train = X[:train_size], Y[:train_size]\n",
    "    X_test, y_test = X[train_size:], Y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: x + y\n",
    "X_train, y_train, X_test, y_test = dataset(1000, 200, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([ 5.6294,  3.7205])\ty = tensor([ 9.3499])\n",
      "X = tensor([ 3.3979,  0.0743])\ty = tensor([ 3.4722])\n",
      "X = tensor([ 2.2337,  0.3268])\ty = tensor([ 2.5605])\n",
      "X = tensor([ 4.2039,  9.6030])\ty = tensor([ 13.8069])\n",
      "X = tensor([ 1.9430,  1.5801])\ty = tensor([ 3.5230])\n",
      "X = tensor([ 7.1482,  3.1841])\ty = tensor([ 10.3323])\n",
      "X = tensor([ 7.6909,  8.4041])\ty = tensor([ 16.0950])\n",
      "X = tensor([ 5.4284,  2.1688])\ty = tensor([ 7.5972])\n",
      "X = tensor([ 3.5210,  7.8779])\ty = tensor([ 11.3989])\n",
      "X = tensor([ 5.3086,  8.7236])\ty = tensor([ 14.0322])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'X = {X_train[i]}\\ty = {y_train[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nac = NeuralAccumulator(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.8147])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nac(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(nac.parameters(), lr=1e-2)\n",
    "crieterion = criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1/1000: loss: 66.5945282 \n",
      "\t2/1000: loss: 2.5424151 \n",
      "\t3/1000: loss: 0.5567204 \n",
      "\t4/1000: loss: 0.3531415 \n",
      "\t5/1000: loss: 0.2442876 \n",
      "\t6/1000: loss: 0.1696015 \n",
      "\t7/1000: loss: 0.1177661 \n",
      "\t8/1000: loss: 0.0817736 \n",
      "\t9/1000: loss: 0.0567815 \n",
      "\t10/1000: loss: 0.0394277 \n",
      "\t11/1000: loss: 0.0273775 \n",
      "\t12/1000: loss: 0.0190103 \n",
      "\t13/1000: loss: 0.0132002 \n",
      "\t14/1000: loss: 0.0091659 \n",
      "\t15/1000: loss: 0.0063645 \n",
      "\t16/1000: loss: 0.0044194 \n",
      "\t17/1000: loss: 0.0030687 \n",
      "\t18/1000: loss: 0.0021308 \n",
      "\t19/1000: loss: 0.0014796 \n",
      "\t20/1000: loss: 0.0010274 \n",
      "\t21/1000: loss: 0.0007134 \n",
      "\t22/1000: loss: 0.0004954 \n",
      "\t23/1000: loss: 0.0003440 \n",
      "\t24/1000: loss: 0.0002388 \n",
      "\t25/1000: loss: 0.0001658 \n",
      "\t26/1000: loss: 0.0001152 \n",
      "\t27/1000: loss: 0.0000800 \n",
      "\t28/1000: loss: 0.0000555 \n",
      "\t29/1000: loss: 0.0000386 \n",
      "\t30/1000: loss: 0.0000268 \n",
      "\t31/1000: loss: 0.0000186 \n",
      "\t32/1000: loss: 0.0000129 \n",
      "\t33/1000: loss: 0.0000090 \n",
      "\t34/1000: loss: 0.0000062 \n",
      "\t35/1000: loss: 0.0000043 \n",
      "\t36/1000: loss: 0.0000030 \n",
      "\t37/1000: loss: 0.0000021 \n",
      "\t38/1000: loss: 0.0000014 \n",
      "\t39/1000: loss: 0.0000010 \n",
      "\t40/1000: loss: 0.0000007 \n",
      "\t41/1000: loss: 0.0000005 \n",
      "\t42/1000: loss: 0.0000003 \n",
      "\t43/1000: loss: 0.0000002 \n",
      "\t44/1000: loss: 0.0000002 \n",
      "\t45/1000: loss: 0.0000001 \n",
      "\t46/1000: loss: 0.0000001 \n",
      "\t47/1000: loss: 0.0000001 \n",
      "\t48/1000: loss: 0.0000000 \n",
      "\t49/1000: loss: 0.0000000 \n",
      "\t50/1000: loss: 0.0000000 \n",
      "\t51/1000: loss: 0.0000000 \n",
      "\t52/1000: loss: 0.0000000 \n",
      "\t53/1000: loss: 0.0000000 \n",
      "\t54/1000: loss: 0.0000000 \n",
      "\t55/1000: loss: 0.0000000 \n",
      "\t56/1000: loss: 0.0000000 \n",
      "\t57/1000: loss: 0.0000000 \n",
      "\t58/1000: loss: 0.0000000 \n",
      "\t59/1000: loss: 0.0000000 \n",
      "\t60/1000: loss: 0.0000000 \n",
      "\t61/1000: loss: 0.0000000 \n",
      "\t62/1000: loss: 0.0000000 \n",
      "\t63/1000: loss: 0.0000000 \n",
      "\t64/1000: loss: 0.0000000 \n",
      "\t65/1000: loss: 0.0000000 \n",
      "\t66/1000: loss: 0.0000000 \n",
      "\t67/1000: loss: 0.0000000 \n",
      "\t68/1000: loss: 0.0000000 \n",
      "\t69/1000: loss: 0.0000000 \n",
      "\t70/1000: loss: 0.0000000 \n",
      "\t71/1000: loss: 0.0000000 \n",
      "\t72/1000: loss: 0.0000000 \n",
      "\t73/1000: loss: 0.0000000 \n",
      "\t74/1000: loss: 0.0000000 \n",
      "\t75/1000: loss: 0.0000000 \n",
      "\t76/1000: loss: 0.0000000 \n",
      "\t77/1000: loss: 0.0000000 \n",
      "\t78/1000: loss: 0.0000000 \n",
      "\t79/1000: loss: 0.0000000 \n",
      "\t80/1000: loss: 0.0000000 \n",
      "\t81/1000: loss: 0.0000000 \n",
      "\t82/1000: loss: 0.0000000 \n",
      "\t83/1000: loss: 0.0000000 \n",
      "\t84/1000: loss: 0.0000000 \n",
      "\t85/1000: loss: 0.0000000 \n",
      "\t86/1000: loss: 0.0000000 \n",
      "\t87/1000: loss: 0.0000000 \n",
      "\t88/1000: loss: 0.0000000 \n",
      "\t89/1000: loss: 0.0000000 \n",
      "\t90/1000: loss: 0.0000000 \n",
      "\t91/1000: loss: 0.0000000 \n",
      "\t92/1000: loss: 0.0000000 \n",
      "\t93/1000: loss: 0.0000000 \n",
      "\t94/1000: loss: 0.0000000 \n",
      "\t95/1000: loss: 0.0000000 \n",
      "\t96/1000: loss: 0.0000000 \n",
      "\t97/1000: loss: 0.0000000 \n",
      "\t98/1000: loss: 0.0000000 \n",
      "\t99/1000: loss: 0.0000000 \n",
      "\t100/1000: loss: 0.0000000 \n",
      "\t101/1000: loss: 0.0000000 \n",
      "\t102/1000: loss: 0.0000000 \n",
      "\t103/1000: loss: 0.0000000 \n",
      "\t104/1000: loss: 0.0000000 \n",
      "\t105/1000: loss: 0.0000000 \n",
      "\t106/1000: loss: 0.0000000 \n",
      "\t107/1000: loss: 0.0000000 \n",
      "\t108/1000: loss: 0.0000000 \n",
      "\t109/1000: loss: 0.0000000 \n",
      "\t110/1000: loss: 0.0000000 \n",
      "\t111/1000: loss: 0.0000000 \n",
      "\t112/1000: loss: 0.0000000 \n",
      "\t113/1000: loss: 0.0000000 \n",
      "\t114/1000: loss: 0.0000000 \n",
      "\t115/1000: loss: 0.0000000 \n",
      "\t116/1000: loss: 0.0000000 \n",
      "\t117/1000: loss: 0.0000000 \n",
      "\t118/1000: loss: 0.0000000 \n",
      "\t119/1000: loss: 0.0000000 \n",
      "\t120/1000: loss: 0.0000000 \n",
      "\t121/1000: loss: 0.0000000 \n",
      "\t122/1000: loss: 0.0000000 \n",
      "\t123/1000: loss: 0.0000000 \n",
      "\t124/1000: loss: 0.0000000 \n",
      "\t125/1000: loss: 0.0000000 \n",
      "\t126/1000: loss: 0.0000000 \n",
      "\t127/1000: loss: 0.0000000 \n",
      "\t128/1000: loss: 0.0000000 \n",
      "\t129/1000: loss: 0.0000000 \n",
      "\t130/1000: loss: 0.0000000 \n",
      "\t131/1000: loss: 0.0000000 \n",
      "\t132/1000: loss: 0.0000000 \n",
      "\t133/1000: loss: 0.0000000 \n",
      "\t134/1000: loss: 0.0000000 \n",
      "\t135/1000: loss: 0.0000000 \n",
      "\t136/1000: loss: 0.0000000 \n",
      "\t137/1000: loss: 0.0000000 \n",
      "\t138/1000: loss: 0.0000000 \n",
      "\t139/1000: loss: 0.0000000 \n",
      "\t140/1000: loss: 0.0000000 \n",
      "\t141/1000: loss: 0.0000000 \n",
      "\t142/1000: loss: 0.0000000 \n",
      "\t143/1000: loss: 0.0000000 \n",
      "\t144/1000: loss: 0.0000000 \n",
      "\t145/1000: loss: 0.0000000 \n",
      "\t146/1000: loss: 0.0000000 \n",
      "\t147/1000: loss: 0.0000000 \n",
      "\t148/1000: loss: 0.0000000 \n",
      "\t149/1000: loss: 0.0000000 \n",
      "\t150/1000: loss: 0.0000000 \n",
      "\t151/1000: loss: 0.0000000 \n",
      "\t152/1000: loss: 0.0000000 \n",
      "\t153/1000: loss: 0.0000000 \n",
      "\t154/1000: loss: 0.0000000 \n",
      "\t155/1000: loss: 0.0000000 \n",
      "\t156/1000: loss: 0.0000000 \n",
      "\t157/1000: loss: 0.0000000 \n",
      "\t158/1000: loss: 0.0000000 \n",
      "\t159/1000: loss: 0.0000000 \n",
      "\t160/1000: loss: 0.0000000 \n",
      "\t161/1000: loss: 0.0000000 \n",
      "\t162/1000: loss: 0.0000000 \n",
      "\t163/1000: loss: 0.0000000 \n",
      "\t164/1000: loss: 0.0000000 \n",
      "\t165/1000: loss: 0.0000000 \n",
      "\t166/1000: loss: 0.0000000 \n",
      "\t167/1000: loss: 0.0000000 \n",
      "\t168/1000: loss: 0.0000000 \n",
      "\t169/1000: loss: 0.0000000 \n",
      "\t170/1000: loss: 0.0000000 \n",
      "\t171/1000: loss: 0.0000000 \n",
      "\t172/1000: loss: 0.0000000 \n",
      "\t173/1000: loss: 0.0000000 \n",
      "\t174/1000: loss: 0.0000000 \n",
      "\t175/1000: loss: 0.0000000 \n",
      "\t176/1000: loss: 0.0000000 \n",
      "\t177/1000: loss: 0.0000000 \n",
      "\t178/1000: loss: 0.0000000 \n",
      "\t179/1000: loss: 0.0000000 \n",
      "\t180/1000: loss: 0.0000000 \n",
      "\t181/1000: loss: 0.0000000 \n",
      "\t182/1000: loss: 0.0000000 \n",
      "\t183/1000: loss: 0.0000000 \n",
      "\t184/1000: loss: 0.0000000 \n",
      "\t185/1000: loss: 0.0000000 \n",
      "\t186/1000: loss: 0.0000000 \n",
      "\t187/1000: loss: 0.0000000 \n",
      "\t188/1000: loss: 0.0000000 \n",
      "\t189/1000: loss: 0.0000000 \n",
      "\t190/1000: loss: 0.0000000 \n",
      "\t191/1000: loss: 0.0000000 \n",
      "\t192/1000: loss: 0.0000000 \n",
      "\t193/1000: loss: 0.0000000 \n",
      "\t194/1000: loss: 0.0000000 \n",
      "\t195/1000: loss: 0.0000000 \n",
      "\t196/1000: loss: 0.0000000 \n",
      "\t197/1000: loss: 0.0000000 \n",
      "\t198/1000: loss: 0.0000000 \n",
      "\t199/1000: loss: 0.0000000 \n",
      "\t200/1000: loss: 0.0000000 \n",
      "\t201/1000: loss: 0.0000000 \n",
      "\t202/1000: loss: 0.0000000 \n",
      "\t203/1000: loss: 0.0000000 \n",
      "\t204/1000: loss: 0.0000000 \n",
      "\t205/1000: loss: 0.0000000 \n",
      "\t206/1000: loss: 0.0000000 \n",
      "\t207/1000: loss: 0.0000000 \n",
      "\t208/1000: loss: 0.0000000 \n",
      "\t209/1000: loss: 0.0000000 \n",
      "\t210/1000: loss: 0.0000000 \n",
      "\t211/1000: loss: 0.0000000 \n",
      "\t212/1000: loss: 0.0000000 \n",
      "\t213/1000: loss: 0.0000000 \n",
      "\t214/1000: loss: 0.0000000 \n",
      "\t215/1000: loss: 0.0000000 \n",
      "\t216/1000: loss: 0.0000000 \n",
      "\t217/1000: loss: 0.0000000 \n",
      "\t218/1000: loss: 0.0000000 \n",
      "\t219/1000: loss: 0.0000000 \n",
      "\t220/1000: loss: 0.0000000 \n",
      "\t221/1000: loss: 0.0000000 \n",
      "\t222/1000: loss: 0.0000000 \n",
      "\t223/1000: loss: 0.0000000 \n",
      "\t224/1000: loss: 0.0000000 \n",
      "\t225/1000: loss: 0.0000000 \n",
      "\t226/1000: loss: 0.0000000 \n",
      "\t227/1000: loss: 0.0000000 \n",
      "\t228/1000: loss: 0.0000000 \n",
      "\t229/1000: loss: 0.0000000 \n",
      "\t230/1000: loss: 0.0000000 \n",
      "\t231/1000: loss: 0.0000000 \n",
      "\t232/1000: loss: 0.0000000 \n",
      "\t233/1000: loss: 0.0000000 \n",
      "\t234/1000: loss: 0.0000000 \n",
      "\t235/1000: loss: 0.0000000 \n",
      "\t236/1000: loss: 0.0000000 \n",
      "\t237/1000: loss: 0.0000000 \n",
      "\t238/1000: loss: 0.0000000 \n",
      "\t239/1000: loss: 0.0000000 \n",
      "\t240/1000: loss: 0.0000000 \n",
      "\t241/1000: loss: 0.0000000 \n",
      "\t242/1000: loss: 0.0000000 \n",
      "\t243/1000: loss: 0.0000000 \n",
      "\t244/1000: loss: 0.0000000 \n",
      "\t245/1000: loss: 0.0000000 \n",
      "\t246/1000: loss: 0.0000000 \n",
      "\t247/1000: loss: 0.0000000 \n",
      "\t248/1000: loss: 0.0000000 \n",
      "\t249/1000: loss: 0.0000000 \n",
      "\t250/1000: loss: 0.0000000 \n",
      "\t251/1000: loss: 0.0000000 \n",
      "\t252/1000: loss: 0.0000000 \n",
      "\t253/1000: loss: 0.0000000 \n",
      "\t254/1000: loss: 0.0000000 \n",
      "\t255/1000: loss: 0.0000000 \n",
      "\t256/1000: loss: 0.0000000 \n",
      "\t257/1000: loss: 0.0000000 \n",
      "\t258/1000: loss: 0.0000000 \n",
      "\t259/1000: loss: 0.0000000 \n",
      "\t260/1000: loss: 0.0000000 \n",
      "\t261/1000: loss: 0.0000000 \n",
      "\t262/1000: loss: 0.0000000 \n",
      "\t263/1000: loss: 0.0000000 \n",
      "\t264/1000: loss: 0.0000000 \n",
      "\t265/1000: loss: 0.0000000 \n",
      "\t266/1000: loss: 0.0000000 \n",
      "\t267/1000: loss: 0.0000000 \n",
      "\t268/1000: loss: 0.0000000 \n",
      "\t269/1000: loss: 0.0000000 \n",
      "\t270/1000: loss: 0.0000000 \n",
      "\t271/1000: loss: 0.0000000 \n",
      "\t272/1000: loss: 0.0000000 \n",
      "\t273/1000: loss: 0.0000000 \n",
      "\t274/1000: loss: 0.0000000 \n",
      "\t275/1000: loss: 0.0000000 \n",
      "\t276/1000: loss: 0.0000000 \n",
      "\t277/1000: loss: 0.0000000 \n",
      "\t278/1000: loss: 0.0000000 \n",
      "\t279/1000: loss: 0.0000000 \n",
      "\t280/1000: loss: 0.0000000 \n",
      "\t281/1000: loss: 0.0000000 \n",
      "\t282/1000: loss: 0.0000000 \n",
      "\t283/1000: loss: 0.0000000 \n",
      "\t284/1000: loss: 0.0000000 \n",
      "\t285/1000: loss: 0.0000000 \n",
      "\t286/1000: loss: 0.0000000 \n",
      "\t287/1000: loss: 0.0000000 \n",
      "\t288/1000: loss: 0.0000000 \n",
      "\t289/1000: loss: 0.0000000 \n",
      "\t290/1000: loss: 0.0000000 \n",
      "\t291/1000: loss: 0.0000000 \n",
      "\t292/1000: loss: 0.0000000 \n",
      "\t293/1000: loss: 0.0000000 \n",
      "\t294/1000: loss: 0.0000000 \n",
      "\t295/1000: loss: 0.0000000 \n",
      "\t296/1000: loss: 0.0000000 \n",
      "\t297/1000: loss: 0.0000000 \n",
      "\t298/1000: loss: 0.0000000 \n",
      "\t299/1000: loss: 0.0000000 \n",
      "\t300/1000: loss: 0.0000000 \n",
      "\t301/1000: loss: 0.0000000 \n",
      "\t302/1000: loss: 0.0000000 \n",
      "\t303/1000: loss: 0.0000000 \n",
      "\t304/1000: loss: 0.0000000 \n",
      "\t305/1000: loss: 0.0000000 \n",
      "\t306/1000: loss: 0.0000000 \n",
      "\t307/1000: loss: 0.0000000 \n",
      "\t308/1000: loss: 0.0000000 \n",
      "\t309/1000: loss: 0.0000000 \n",
      "\t310/1000: loss: 0.0000000 \n",
      "\t311/1000: loss: 0.0000000 \n",
      "\t312/1000: loss: 0.0000000 \n",
      "\t313/1000: loss: 0.0000000 \n",
      "\t314/1000: loss: 0.0000000 \n",
      "\t315/1000: loss: 0.0000000 \n",
      "\t316/1000: loss: 0.0000000 \n",
      "\t317/1000: loss: 0.0000000 \n",
      "\t318/1000: loss: 0.0000000 \n",
      "\t319/1000: loss: 0.0000000 \n",
      "\t320/1000: loss: 0.0000000 \n",
      "\t321/1000: loss: 0.0000000 \n",
      "\t322/1000: loss: 0.0000000 \n",
      "\t323/1000: loss: 0.0000000 \n",
      "\t324/1000: loss: 0.0000000 \n",
      "\t325/1000: loss: 0.0000000 \n",
      "\t326/1000: loss: 0.0000000 \n",
      "\t327/1000: loss: 0.0000000 \n",
      "\t328/1000: loss: 0.0000000 \n",
      "\t329/1000: loss: 0.0000000 \n",
      "\t330/1000: loss: 0.0000000 \n",
      "\t331/1000: loss: 0.0000000 \n",
      "\t332/1000: loss: 0.0000000 \n",
      "\t333/1000: loss: 0.0000000 \n",
      "\t334/1000: loss: 0.0000000 \n",
      "\t335/1000: loss: 0.0000000 \n",
      "\t336/1000: loss: 0.0000000 \n",
      "\t337/1000: loss: 0.0000000 \n",
      "\t338/1000: loss: 0.0000000 \n",
      "\t339/1000: loss: 0.0000000 \n",
      "\t340/1000: loss: 0.0000000 \n",
      "\t341/1000: loss: 0.0000000 \n",
      "\t342/1000: loss: 0.0000000 \n",
      "\t343/1000: loss: 0.0000000 \n",
      "\t344/1000: loss: 0.0000000 \n",
      "\t345/1000: loss: 0.0000000 \n",
      "\t346/1000: loss: 0.0000000 \n",
      "\t347/1000: loss: 0.0000000 \n",
      "\t348/1000: loss: 0.0000000 \n",
      "\t349/1000: loss: 0.0000000 \n",
      "\t350/1000: loss: 0.0000000 \n",
      "\t351/1000: loss: 0.0000000 \n",
      "\t352/1000: loss: 0.0000000 \n",
      "\t353/1000: loss: 0.0000000 \n",
      "\t354/1000: loss: 0.0000000 \n",
      "\t355/1000: loss: 0.0000000 \n",
      "\t356/1000: loss: 0.0000000 \n",
      "\t357/1000: loss: 0.0000000 \n",
      "\t358/1000: loss: 0.0000000 \n",
      "\t359/1000: loss: 0.0000000 \n",
      "\t360/1000: loss: 0.0000000 \n",
      "\t361/1000: loss: 0.0000000 \n",
      "\t362/1000: loss: 0.0000000 \n",
      "\t363/1000: loss: 0.0000000 \n",
      "\t364/1000: loss: 0.0000000 \n",
      "\t365/1000: loss: 0.0000000 \n",
      "\t366/1000: loss: 0.0000000 \n",
      "\t367/1000: loss: 0.0000000 \n",
      "\t368/1000: loss: 0.0000000 \n",
      "\t369/1000: loss: 0.0000000 \n",
      "\t370/1000: loss: 0.0000000 \n",
      "\t371/1000: loss: 0.0000000 \n",
      "\t372/1000: loss: 0.0000000 \n",
      "\t373/1000: loss: 0.0000000 \n",
      "\t374/1000: loss: 0.0000000 \n",
      "\t375/1000: loss: 0.0000000 \n",
      "\t376/1000: loss: 0.0000000 \n",
      "\t377/1000: loss: 0.0000000 \n",
      "\t378/1000: loss: 0.0000000 \n",
      "\t379/1000: loss: 0.0000000 \n",
      "\t380/1000: loss: 0.0000000 \n",
      "\t381/1000: loss: 0.0000000 \n",
      "\t382/1000: loss: 0.0000000 \n",
      "\t383/1000: loss: 0.0000000 \n",
      "\t384/1000: loss: 0.0000000 \n",
      "\t385/1000: loss: 0.0000000 \n",
      "\t386/1000: loss: 0.0000000 \n",
      "\t387/1000: loss: 0.0000000 \n",
      "\t388/1000: loss: 0.0000000 \n",
      "\t389/1000: loss: 0.0000000 \n",
      "\t390/1000: loss: 0.0000000 \n",
      "\t391/1000: loss: 0.0000000 \n",
      "\t392/1000: loss: 0.0000000 \n",
      "\t393/1000: loss: 0.0000000 \n",
      "\t394/1000: loss: 0.0000000 \n",
      "\t395/1000: loss: 0.0000000 \n",
      "\t396/1000: loss: 0.0000000 \n",
      "\t397/1000: loss: 0.0000000 \n",
      "\t398/1000: loss: 0.0000000 \n",
      "\t399/1000: loss: 0.0000000 \n",
      "\t400/1000: loss: 0.0000000 \n",
      "\t401/1000: loss: 0.0000000 \n",
      "\t402/1000: loss: 0.0000000 \n",
      "\t403/1000: loss: 0.0000000 \n",
      "\t404/1000: loss: 0.0000000 \n",
      "\t405/1000: loss: 0.0000000 \n",
      "\t406/1000: loss: 0.0000000 \n",
      "\t407/1000: loss: 0.0000000 \n",
      "\t408/1000: loss: 0.0000000 \n",
      "\t409/1000: loss: 0.0000000 \n",
      "\t410/1000: loss: 0.0000000 \n",
      "\t411/1000: loss: 0.0000000 \n",
      "\t412/1000: loss: 0.0000000 \n",
      "\t413/1000: loss: 0.0000000 \n",
      "\t414/1000: loss: 0.0000000 \n",
      "\t415/1000: loss: 0.0000000 \n",
      "\t416/1000: loss: 0.0000000 \n",
      "\t417/1000: loss: 0.0000000 \n",
      "\t418/1000: loss: 0.0000000 \n",
      "\t419/1000: loss: 0.0000000 \n",
      "\t420/1000: loss: 0.0000000 \n",
      "\t421/1000: loss: 0.0000000 \n",
      "\t422/1000: loss: 0.0000000 \n",
      "\t423/1000: loss: 0.0000000 \n",
      "\t424/1000: loss: 0.0000000 \n",
      "\t425/1000: loss: 0.0000000 \n",
      "\t426/1000: loss: 0.0000000 \n",
      "\t427/1000: loss: 0.0000000 \n",
      "\t428/1000: loss: 0.0000000 \n",
      "\t429/1000: loss: 0.0000000 \n",
      "\t430/1000: loss: 0.0000000 \n",
      "\t431/1000: loss: 0.0000000 \n",
      "\t432/1000: loss: 0.0000000 \n",
      "\t433/1000: loss: 0.0000000 \n",
      "\t434/1000: loss: 0.0000000 \n",
      "\t435/1000: loss: 0.0000000 \n",
      "\t436/1000: loss: 0.0000000 \n",
      "\t437/1000: loss: 0.0000000 \n",
      "\t438/1000: loss: 0.0000000 \n",
      "\t439/1000: loss: 0.0000000 \n",
      "\t440/1000: loss: 0.0000000 \n",
      "\t441/1000: loss: 0.0000000 \n",
      "\t442/1000: loss: 0.0000000 \n",
      "\t443/1000: loss: 0.0000000 \n",
      "\t444/1000: loss: 0.0000000 \n",
      "\t445/1000: loss: 0.0000000 \n",
      "\t446/1000: loss: 0.0000000 \n",
      "\t447/1000: loss: 0.0000000 \n",
      "\t448/1000: loss: 0.0000000 \n",
      "\t449/1000: loss: 0.0000000 \n",
      "\t450/1000: loss: 0.0000000 \n",
      "\t451/1000: loss: 0.0000000 \n",
      "\t452/1000: loss: 0.0000000 \n",
      "\t453/1000: loss: 0.0000000 \n",
      "\t454/1000: loss: 0.0000000 \n",
      "\t455/1000: loss: 0.0000000 \n",
      "\t456/1000: loss: 0.0000000 \n",
      "\t457/1000: loss: 0.0000000 \n",
      "\t458/1000: loss: 0.0000000 \n",
      "\t459/1000: loss: 0.0000000 \n",
      "\t460/1000: loss: 0.0000000 \n",
      "\t461/1000: loss: 0.0000000 \n",
      "\t462/1000: loss: 0.0000000 \n",
      "\t463/1000: loss: 0.0000000 \n",
      "\t464/1000: loss: 0.0000000 \n",
      "\t465/1000: loss: 0.0000000 \n",
      "\t466/1000: loss: 0.0000000 \n",
      "\t467/1000: loss: 0.0000000 \n",
      "\t468/1000: loss: 0.0000000 \n",
      "\t469/1000: loss: 0.0000000 \n",
      "\t470/1000: loss: 0.0000000 \n",
      "\t471/1000: loss: 0.0000000 \n",
      "\t472/1000: loss: 0.0000000 \n",
      "\t473/1000: loss: 0.0000000 \n",
      "\t474/1000: loss: 0.0000000 \n",
      "\t475/1000: loss: 0.0000000 \n",
      "\t476/1000: loss: 0.0000000 \n",
      "\t477/1000: loss: 0.0000000 \n",
      "\t478/1000: loss: 0.0000000 \n",
      "\t479/1000: loss: 0.0000000 \n",
      "\t480/1000: loss: 0.0000000 \n",
      "\t481/1000: loss: 0.0000000 \n",
      "\t482/1000: loss: 0.0000000 \n",
      "\t483/1000: loss: 0.0000000 \n",
      "\t484/1000: loss: 0.0000000 \n",
      "\t485/1000: loss: 0.0000000 \n",
      "\t486/1000: loss: 0.0000000 \n",
      "\t487/1000: loss: 0.0000000 \n",
      "\t488/1000: loss: 0.0000000 \n",
      "\t489/1000: loss: 0.0000000 \n",
      "\t490/1000: loss: 0.0000000 \n",
      "\t491/1000: loss: 0.0000000 \n",
      "\t492/1000: loss: 0.0000000 \n",
      "\t493/1000: loss: 0.0000000 \n",
      "\t494/1000: loss: 0.0000000 \n",
      "\t495/1000: loss: 0.0000000 \n",
      "\t496/1000: loss: 0.0000000 \n",
      "\t497/1000: loss: 0.0000000 \n",
      "\t498/1000: loss: 0.0000000 \n",
      "\t499/1000: loss: 0.0000000 \n",
      "\t500/1000: loss: 0.0000000 \n",
      "\t501/1000: loss: 0.0000000 \n",
      "\t502/1000: loss: 0.0000000 \n",
      "\t503/1000: loss: 0.0000000 \n",
      "\t504/1000: loss: 0.0000000 \n",
      "\t505/1000: loss: 0.0000000 \n",
      "\t506/1000: loss: 0.0000000 \n",
      "\t507/1000: loss: 0.0000000 \n",
      "\t508/1000: loss: 0.0000000 \n",
      "\t509/1000: loss: 0.0000000 \n",
      "\t510/1000: loss: 0.0000000 \n",
      "\t511/1000: loss: 0.0000000 \n",
      "\t512/1000: loss: 0.0000000 \n",
      "\t513/1000: loss: 0.0000000 \n",
      "\t514/1000: loss: 0.0000000 \n",
      "\t515/1000: loss: 0.0000000 \n",
      "\t516/1000: loss: 0.0000000 \n",
      "\t517/1000: loss: 0.0000000 \n",
      "\t518/1000: loss: 0.0000000 \n",
      "\t519/1000: loss: 0.0000000 \n",
      "\t520/1000: loss: 0.0000000 \n",
      "\t521/1000: loss: 0.0000000 \n",
      "\t522/1000: loss: 0.0000000 \n",
      "\t523/1000: loss: 0.0000000 \n",
      "\t524/1000: loss: 0.0000000 \n",
      "\t525/1000: loss: 0.0000000 \n",
      "\t526/1000: loss: 0.0000000 \n",
      "\t527/1000: loss: 0.0000000 \n",
      "\t528/1000: loss: 0.0000000 \n",
      "\t529/1000: loss: 0.0000000 \n",
      "\t530/1000: loss: 0.0000000 \n",
      "\t531/1000: loss: 0.0000000 \n",
      "\t532/1000: loss: 0.0000000 \n",
      "\t533/1000: loss: 0.0000000 \n",
      "\t534/1000: loss: 0.0000000 \n",
      "\t535/1000: loss: 0.0000000 \n",
      "\t536/1000: loss: 0.0000000 \n",
      "\t537/1000: loss: 0.0000000 \n",
      "\t538/1000: loss: 0.0000000 \n",
      "\t539/1000: loss: 0.0000000 \n",
      "\t540/1000: loss: 0.0000000 \n",
      "\t541/1000: loss: 0.0000000 \n",
      "\t542/1000: loss: 0.0000000 \n",
      "\t543/1000: loss: 0.0000000 \n",
      "\t544/1000: loss: 0.0000000 \n",
      "\t545/1000: loss: 0.0000000 \n",
      "\t546/1000: loss: 0.0000000 \n",
      "\t547/1000: loss: 0.0000000 \n",
      "\t548/1000: loss: 0.0000000 \n",
      "\t549/1000: loss: 0.0000000 \n",
      "\t550/1000: loss: 0.0000000 \n",
      "\t551/1000: loss: 0.0000000 \n",
      "\t552/1000: loss: 0.0000000 \n",
      "\t553/1000: loss: 0.0000000 \n",
      "\t554/1000: loss: 0.0000000 \n",
      "\t555/1000: loss: 0.0000000 \n",
      "\t556/1000: loss: 0.0000000 \n",
      "\t557/1000: loss: 0.0000000 \n",
      "\t558/1000: loss: 0.0000000 \n",
      "\t559/1000: loss: 0.0000000 \n",
      "\t560/1000: loss: 0.0000000 \n",
      "\t561/1000: loss: 0.0000000 \n",
      "\t562/1000: loss: 0.0000000 \n",
      "\t563/1000: loss: 0.0000000 \n",
      "\t564/1000: loss: 0.0000000 \n",
      "\t565/1000: loss: 0.0000000 \n",
      "\t566/1000: loss: 0.0000000 \n",
      "\t567/1000: loss: 0.0000000 \n",
      "\t568/1000: loss: 0.0000000 \n",
      "\t569/1000: loss: 0.0000000 \n",
      "\t570/1000: loss: 0.0000000 \n",
      "\t571/1000: loss: 0.0000000 \n",
      "\t572/1000: loss: 0.0000000 \n",
      "\t573/1000: loss: 0.0000000 \n",
      "\t574/1000: loss: 0.0000000 \n",
      "\t575/1000: loss: 0.0000000 \n",
      "\t576/1000: loss: 0.0000000 \n",
      "\t577/1000: loss: 0.0000000 \n",
      "\t578/1000: loss: 0.0000000 \n",
      "\t579/1000: loss: 0.0000000 \n",
      "\t580/1000: loss: 0.0000000 \n",
      "\t581/1000: loss: 0.0000000 \n",
      "\t582/1000: loss: 0.0000000 \n",
      "\t583/1000: loss: 0.0000000 \n",
      "\t584/1000: loss: 0.0000000 \n",
      "\t585/1000: loss: 0.0000000 \n",
      "\t586/1000: loss: 0.0000000 \n",
      "\t587/1000: loss: 0.0000000 \n",
      "\t588/1000: loss: 0.0000000 \n",
      "\t589/1000: loss: 0.0000000 \n",
      "\t590/1000: loss: 0.0000000 \n",
      "\t591/1000: loss: 0.0000000 \n",
      "\t592/1000: loss: 0.0000000 \n",
      "\t593/1000: loss: 0.0000000 \n",
      "\t594/1000: loss: 0.0000000 \n",
      "\t595/1000: loss: 0.0000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t596/1000: loss: 0.0000000 \n",
      "\t597/1000: loss: 0.0000000 \n",
      "\t598/1000: loss: 0.0000000 \n",
      "\t599/1000: loss: 0.0000000 \n",
      "\t600/1000: loss: 0.0000000 \n",
      "\t601/1000: loss: 0.0000000 \n",
      "\t602/1000: loss: 0.0000000 \n",
      "\t603/1000: loss: 0.0000000 \n",
      "\t604/1000: loss: 0.0000000 \n",
      "\t605/1000: loss: 0.0000000 \n",
      "\t606/1000: loss: 0.0000000 \n",
      "\t607/1000: loss: 0.0000000 \n",
      "\t608/1000: loss: 0.0000000 \n",
      "\t609/1000: loss: 0.0000000 \n",
      "\t610/1000: loss: 0.0000000 \n",
      "\t611/1000: loss: 0.0000000 \n",
      "\t612/1000: loss: 0.0000000 \n",
      "\t613/1000: loss: 0.0000000 \n",
      "\t614/1000: loss: 0.0000000 \n",
      "\t615/1000: loss: 0.0000000 \n",
      "\t616/1000: loss: 0.0000000 \n",
      "\t617/1000: loss: 0.0000000 \n",
      "\t618/1000: loss: 0.0000000 \n",
      "\t619/1000: loss: 0.0000000 \n",
      "\t620/1000: loss: 0.0000000 \n",
      "\t621/1000: loss: 0.0000000 \n",
      "\t622/1000: loss: 0.0000000 \n",
      "\t623/1000: loss: 0.0000000 \n",
      "\t624/1000: loss: 0.0000000 \n",
      "\t625/1000: loss: 0.0000000 \n",
      "\t626/1000: loss: 0.0000000 \n",
      "\t627/1000: loss: 0.0000000 \n",
      "\t628/1000: loss: 0.0000000 \n",
      "\t629/1000: loss: 0.0000000 \n",
      "\t630/1000: loss: 0.0000000 \n",
      "\t631/1000: loss: 0.0000000 \n",
      "\t632/1000: loss: 0.0000000 \n",
      "\t633/1000: loss: 0.0000000 \n",
      "\t634/1000: loss: 0.0000000 \n",
      "\t635/1000: loss: 0.0000000 \n",
      "\t636/1000: loss: 0.0000000 \n",
      "\t637/1000: loss: 0.0000000 \n",
      "\t638/1000: loss: 0.0000000 \n",
      "\t639/1000: loss: 0.0000000 \n",
      "\t640/1000: loss: 0.0000000 \n",
      "\t641/1000: loss: 0.0000000 \n",
      "\t642/1000: loss: 0.0000000 \n",
      "\t643/1000: loss: 0.0000000 \n",
      "\t644/1000: loss: 0.0000000 \n",
      "\t645/1000: loss: 0.0000000 \n",
      "\t646/1000: loss: 0.0000000 \n",
      "\t647/1000: loss: 0.0000000 \n",
      "\t648/1000: loss: 0.0000000 \n",
      "\t649/1000: loss: 0.0000000 \n",
      "\t650/1000: loss: 0.0000000 \n",
      "\t651/1000: loss: 0.0000000 \n",
      "\t652/1000: loss: 0.0000000 \n",
      "\t653/1000: loss: 0.0000000 \n",
      "\t654/1000: loss: 0.0000000 \n",
      "\t655/1000: loss: 0.0000000 \n",
      "\t656/1000: loss: 0.0000000 \n",
      "\t657/1000: loss: 0.0000000 \n",
      "\t658/1000: loss: 0.0000000 \n",
      "\t659/1000: loss: 0.0000000 \n",
      "\t660/1000: loss: 0.0000000 \n",
      "\t661/1000: loss: 0.0000000 \n",
      "\t662/1000: loss: 0.0000000 \n",
      "\t663/1000: loss: 0.0000000 \n",
      "\t664/1000: loss: 0.0000000 \n",
      "\t665/1000: loss: 0.0000000 \n",
      "\t666/1000: loss: 0.0000000 \n",
      "\t667/1000: loss: 0.0000000 \n",
      "\t668/1000: loss: 0.0000000 \n",
      "\t669/1000: loss: 0.0000000 \n",
      "\t670/1000: loss: 0.0000000 \n",
      "\t671/1000: loss: 0.0000000 \n",
      "\t672/1000: loss: 0.0000000 \n",
      "\t673/1000: loss: 0.0000000 \n",
      "\t674/1000: loss: 0.0000000 \n",
      "\t675/1000: loss: 0.0000000 \n",
      "\t676/1000: loss: 0.0000000 \n",
      "\t677/1000: loss: 0.0000000 \n",
      "\t678/1000: loss: 0.0000000 \n",
      "\t679/1000: loss: 0.0000000 \n",
      "\t680/1000: loss: 0.0000000 \n",
      "\t681/1000: loss: 0.0000000 \n",
      "\t682/1000: loss: 0.0000000 \n",
      "\t683/1000: loss: 0.0000000 \n",
      "\t684/1000: loss: 0.0000000 \n",
      "\t685/1000: loss: 0.0000000 \n",
      "\t686/1000: loss: 0.0000000 \n",
      "\t687/1000: loss: 0.0000000 \n",
      "\t688/1000: loss: 0.0000000 \n",
      "\t689/1000: loss: 0.0000000 \n",
      "\t690/1000: loss: 0.0000000 \n",
      "\t691/1000: loss: 0.0000000 \n",
      "\t692/1000: loss: 0.0000000 \n",
      "\t693/1000: loss: 0.0000000 \n",
      "\t694/1000: loss: 0.0000000 \n",
      "\t695/1000: loss: 0.0000000 \n",
      "\t696/1000: loss: 0.0000000 \n",
      "\t697/1000: loss: 0.0000000 \n",
      "\t698/1000: loss: 0.0000000 \n",
      "\t699/1000: loss: 0.0000000 \n",
      "\t700/1000: loss: 0.0000000 \n",
      "\t701/1000: loss: 0.0000000 \n",
      "\t702/1000: loss: 0.0000000 \n",
      "\t703/1000: loss: 0.0000000 \n",
      "\t704/1000: loss: 0.0000000 \n",
      "\t705/1000: loss: 0.0000000 \n",
      "\t706/1000: loss: 0.0000000 \n",
      "\t707/1000: loss: 0.0000000 \n",
      "\t708/1000: loss: 0.0000000 \n",
      "\t709/1000: loss: 0.0000000 \n",
      "\t710/1000: loss: 0.0000000 \n",
      "\t711/1000: loss: 0.0000000 \n",
      "\t712/1000: loss: 0.0000000 \n",
      "\t713/1000: loss: 0.0000000 \n",
      "\t714/1000: loss: 0.0000000 \n",
      "\t715/1000: loss: 0.0000000 \n",
      "\t716/1000: loss: 0.0000000 \n",
      "\t717/1000: loss: 0.0000000 \n",
      "\t718/1000: loss: 0.0000000 \n",
      "\t719/1000: loss: 0.0000000 \n",
      "\t720/1000: loss: 0.0000000 \n",
      "\t721/1000: loss: 0.0000000 \n",
      "\t722/1000: loss: 0.0000000 \n",
      "\t723/1000: loss: 0.0000000 \n",
      "\t724/1000: loss: 0.0000000 \n",
      "\t725/1000: loss: 0.0000000 \n",
      "\t726/1000: loss: 0.0000000 \n",
      "\t727/1000: loss: 0.0000000 \n",
      "\t728/1000: loss: 0.0000000 \n",
      "\t729/1000: loss: 0.0000000 \n",
      "\t730/1000: loss: 0.0000000 \n",
      "\t731/1000: loss: 0.0000000 \n",
      "\t732/1000: loss: 0.0000000 \n",
      "\t733/1000: loss: 0.0000000 \n",
      "\t734/1000: loss: 0.0000000 \n",
      "\t735/1000: loss: 0.0000000 \n",
      "\t736/1000: loss: 0.0000000 \n",
      "\t737/1000: loss: 0.0000000 \n",
      "\t738/1000: loss: 0.0000000 \n",
      "\t739/1000: loss: 0.0000000 \n",
      "\t740/1000: loss: 0.0000000 \n",
      "\t741/1000: loss: 0.0000000 \n",
      "\t742/1000: loss: 0.0000000 \n",
      "\t743/1000: loss: 0.0000000 \n",
      "\t744/1000: loss: 0.0000000 \n",
      "\t745/1000: loss: 0.0000000 \n",
      "\t746/1000: loss: 0.0000000 \n",
      "\t747/1000: loss: 0.0000000 \n",
      "\t748/1000: loss: 0.0000000 \n",
      "\t749/1000: loss: 0.0000000 \n",
      "\t750/1000: loss: 0.0000000 \n",
      "\t751/1000: loss: 0.0000000 \n",
      "\t752/1000: loss: 0.0000000 \n",
      "\t753/1000: loss: 0.0000000 \n",
      "\t754/1000: loss: 0.0000000 \n",
      "\t755/1000: loss: 0.0000000 \n",
      "\t756/1000: loss: 0.0000000 \n",
      "\t757/1000: loss: 0.0000000 \n",
      "\t758/1000: loss: 0.0000000 \n",
      "\t759/1000: loss: 0.0000000 \n",
      "\t760/1000: loss: 0.0000000 \n",
      "\t761/1000: loss: 0.0000000 \n",
      "\t762/1000: loss: 0.0000000 \n",
      "\t763/1000: loss: 0.0000000 \n",
      "\t764/1000: loss: 0.0000000 \n",
      "\t765/1000: loss: 0.0000000 \n",
      "\t766/1000: loss: 0.0000000 \n",
      "\t767/1000: loss: 0.0000000 \n",
      "\t768/1000: loss: 0.0000000 \n",
      "\t769/1000: loss: 0.0000000 \n",
      "\t770/1000: loss: 0.0000000 \n",
      "\t771/1000: loss: 0.0000000 \n",
      "\t772/1000: loss: 0.0000000 \n",
      "\t773/1000: loss: 0.0000000 \n",
      "\t774/1000: loss: 0.0000000 \n",
      "\t775/1000: loss: 0.0000000 \n",
      "\t776/1000: loss: 0.0000000 \n",
      "\t777/1000: loss: 0.0000000 \n",
      "\t778/1000: loss: 0.0000000 \n",
      "\t779/1000: loss: 0.0000000 \n",
      "\t780/1000: loss: 0.0000000 \n",
      "\t781/1000: loss: 0.0000000 \n",
      "\t782/1000: loss: 0.0000000 \n",
      "\t783/1000: loss: 0.0000000 \n",
      "\t784/1000: loss: 0.0000000 \n",
      "\t785/1000: loss: 0.0000000 \n",
      "\t786/1000: loss: 0.0000000 \n",
      "\t787/1000: loss: 0.0000000 \n",
      "\t788/1000: loss: 0.0000000 \n",
      "\t789/1000: loss: 0.0000000 \n",
      "\t790/1000: loss: 0.0000000 \n",
      "\t791/1000: loss: 0.0000000 \n",
      "\t792/1000: loss: 0.0000000 \n",
      "\t793/1000: loss: 0.0000000 \n",
      "\t794/1000: loss: 0.0000000 \n",
      "\t795/1000: loss: 0.0000000 \n",
      "\t796/1000: loss: 0.0000000 \n",
      "\t797/1000: loss: 0.0000000 \n",
      "\t798/1000: loss: 0.0000000 \n",
      "\t799/1000: loss: 0.0000000 \n",
      "\t800/1000: loss: 0.0000000 \n",
      "\t801/1000: loss: 0.0000000 \n",
      "\t802/1000: loss: 0.0000000 \n",
      "\t803/1000: loss: 0.0000000 \n",
      "\t804/1000: loss: 0.0000000 \n",
      "\t805/1000: loss: 0.0000000 \n",
      "\t806/1000: loss: 0.0000000 \n",
      "\t807/1000: loss: 0.0000000 \n",
      "\t808/1000: loss: 0.0000000 \n",
      "\t809/1000: loss: 0.0000000 \n",
      "\t810/1000: loss: 0.0000000 \n",
      "\t811/1000: loss: 0.0000000 \n",
      "\t812/1000: loss: 0.0000000 \n",
      "\t813/1000: loss: 0.0000000 \n",
      "\t814/1000: loss: 0.0000000 \n",
      "\t815/1000: loss: 0.0000000 \n",
      "\t816/1000: loss: 0.0000000 \n",
      "\t817/1000: loss: 0.0000000 \n",
      "\t818/1000: loss: 0.0000000 \n",
      "\t819/1000: loss: 0.0000000 \n",
      "\t820/1000: loss: 0.0000000 \n",
      "\t821/1000: loss: 0.0000000 \n",
      "\t822/1000: loss: 0.0000000 \n",
      "\t823/1000: loss: 0.0000000 \n",
      "\t824/1000: loss: 0.0000000 \n",
      "\t825/1000: loss: 0.0000000 \n",
      "\t826/1000: loss: 0.0000000 \n",
      "\t827/1000: loss: 0.0000000 \n",
      "\t828/1000: loss: 0.0000000 \n",
      "\t829/1000: loss: 0.0000000 \n",
      "\t830/1000: loss: 0.0000000 \n",
      "\t831/1000: loss: 0.0000000 \n",
      "\t832/1000: loss: 0.0000000 \n",
      "\t833/1000: loss: 0.0000000 \n",
      "\t834/1000: loss: 0.0000000 \n",
      "\t835/1000: loss: 0.0000000 \n",
      "\t836/1000: loss: 0.0000000 \n",
      "\t837/1000: loss: 0.0000000 \n",
      "\t838/1000: loss: 0.0000000 \n",
      "\t839/1000: loss: 0.0000000 \n",
      "\t840/1000: loss: 0.0000000 \n",
      "\t841/1000: loss: 0.0000000 \n",
      "\t842/1000: loss: 0.0000000 \n",
      "\t843/1000: loss: 0.0000000 \n",
      "\t844/1000: loss: 0.0000000 \n",
      "\t845/1000: loss: 0.0000000 \n",
      "\t846/1000: loss: 0.0000000 \n",
      "\t847/1000: loss: 0.0000000 \n",
      "\t848/1000: loss: 0.0000000 \n",
      "\t849/1000: loss: 0.0000000 \n",
      "\t850/1000: loss: 0.0000000 \n",
      "\t851/1000: loss: 0.0000000 \n",
      "\t852/1000: loss: 0.0000000 \n",
      "\t853/1000: loss: 0.0000000 \n",
      "\t854/1000: loss: 0.0000000 \n",
      "\t855/1000: loss: 0.0000000 \n",
      "\t856/1000: loss: 0.0000000 \n",
      "\t857/1000: loss: 0.0000000 \n",
      "\t858/1000: loss: 0.0000000 \n",
      "\t859/1000: loss: 0.0000000 \n",
      "\t860/1000: loss: 0.0000000 \n",
      "\t861/1000: loss: 0.0000000 \n",
      "\t862/1000: loss: 0.0000000 \n",
      "\t863/1000: loss: 0.0000000 \n",
      "\t864/1000: loss: 0.0000000 \n",
      "\t865/1000: loss: 0.0000000 \n",
      "\t866/1000: loss: 0.0000000 \n",
      "\t867/1000: loss: 0.0000000 \n",
      "\t868/1000: loss: 0.0000000 \n",
      "\t869/1000: loss: 0.0000000 \n",
      "\t870/1000: loss: 0.0000000 \n",
      "\t871/1000: loss: 0.0000000 \n",
      "\t872/1000: loss: 0.0000000 \n",
      "\t873/1000: loss: 0.0000000 \n",
      "\t874/1000: loss: 0.0000000 \n",
      "\t875/1000: loss: 0.0000000 \n",
      "\t876/1000: loss: 0.0000000 \n",
      "\t877/1000: loss: 0.0000000 \n",
      "\t878/1000: loss: 0.0000000 \n",
      "\t879/1000: loss: 0.0000000 \n",
      "\t880/1000: loss: 0.0000000 \n",
      "\t881/1000: loss: 0.0000000 \n",
      "\t882/1000: loss: 0.0000000 \n",
      "\t883/1000: loss: 0.0000000 \n",
      "\t884/1000: loss: 0.0000000 \n",
      "\t885/1000: loss: 0.0000000 \n",
      "\t886/1000: loss: 0.0000000 \n",
      "\t887/1000: loss: 0.0000000 \n",
      "\t888/1000: loss: 0.0000000 \n",
      "\t889/1000: loss: 0.0000000 \n",
      "\t890/1000: loss: 0.0000000 \n",
      "\t891/1000: loss: 0.0000000 \n",
      "\t892/1000: loss: 0.0000000 \n",
      "\t893/1000: loss: 0.0000000 \n",
      "\t894/1000: loss: 0.0000000 \n",
      "\t895/1000: loss: 0.0000000 \n",
      "\t896/1000: loss: 0.0000000 \n",
      "\t897/1000: loss: 0.0000000 \n",
      "\t898/1000: loss: 0.0000000 \n",
      "\t899/1000: loss: 0.0000000 \n",
      "\t900/1000: loss: 0.0000000 \n",
      "\t901/1000: loss: 0.0000000 \n",
      "\t902/1000: loss: 0.0000000 \n",
      "\t903/1000: loss: 0.0000000 \n",
      "\t904/1000: loss: 0.0000000 \n",
      "\t905/1000: loss: 0.0000000 \n",
      "\t906/1000: loss: 0.0000000 \n",
      "\t907/1000: loss: 0.0000000 \n",
      "\t908/1000: loss: 0.0000000 \n",
      "\t909/1000: loss: 0.0000000 \n",
      "\t910/1000: loss: 0.0000000 \n",
      "\t911/1000: loss: 0.0000000 \n",
      "\t912/1000: loss: 0.0000000 \n",
      "\t913/1000: loss: 0.0000000 \n",
      "\t914/1000: loss: 0.0000000 \n",
      "\t915/1000: loss: 0.0000000 \n",
      "\t916/1000: loss: 0.0000000 \n",
      "\t917/1000: loss: 0.0000000 \n",
      "\t918/1000: loss: 0.0000000 \n",
      "\t919/1000: loss: 0.0000000 \n",
      "\t920/1000: loss: 0.0000000 \n",
      "\t921/1000: loss: 0.0000000 \n",
      "\t922/1000: loss: 0.0000000 \n",
      "\t923/1000: loss: 0.0000000 \n",
      "\t924/1000: loss: 0.0000000 \n",
      "\t925/1000: loss: 0.0000000 \n",
      "\t926/1000: loss: 0.0000000 \n",
      "\t927/1000: loss: 0.0000000 \n",
      "\t928/1000: loss: 0.0000000 \n",
      "\t929/1000: loss: 0.0000000 \n",
      "\t930/1000: loss: 0.0000000 \n",
      "\t931/1000: loss: 0.0000000 \n",
      "\t932/1000: loss: 0.0000000 \n",
      "\t933/1000: loss: 0.0000000 \n",
      "\t934/1000: loss: 0.0000000 \n",
      "\t935/1000: loss: 0.0000000 \n",
      "\t936/1000: loss: 0.0000000 \n",
      "\t937/1000: loss: 0.0000000 \n",
      "\t938/1000: loss: 0.0000000 \n",
      "\t939/1000: loss: 0.0000000 \n",
      "\t940/1000: loss: 0.0000000 \n",
      "\t941/1000: loss: 0.0000000 \n",
      "\t942/1000: loss: 0.0000000 \n",
      "\t943/1000: loss: 0.0000000 \n",
      "\t944/1000: loss: 0.0000000 \n",
      "\t945/1000: loss: 0.0000000 \n",
      "\t946/1000: loss: 0.0000000 \n",
      "\t947/1000: loss: 0.0000000 \n",
      "\t948/1000: loss: 0.0000000 \n",
      "\t949/1000: loss: 0.0000000 \n",
      "\t950/1000: loss: 0.0000000 \n",
      "\t951/1000: loss: 0.0000000 \n",
      "\t952/1000: loss: 0.0000000 \n",
      "\t953/1000: loss: 0.0000000 \n",
      "\t954/1000: loss: 0.0000000 \n",
      "\t955/1000: loss: 0.0000000 \n",
      "\t956/1000: loss: 0.0000000 \n",
      "\t957/1000: loss: 0.0000000 \n",
      "\t958/1000: loss: 0.0000000 \n",
      "\t959/1000: loss: 0.0000000 \n",
      "\t960/1000: loss: 0.0000000 \n",
      "\t961/1000: loss: 0.0000000 \n",
      "\t962/1000: loss: 0.0000000 \n",
      "\t963/1000: loss: 0.0000000 \n",
      "\t964/1000: loss: 0.0000000 \n",
      "\t965/1000: loss: 0.0000000 \n",
      "\t966/1000: loss: 0.0000000 \n",
      "\t967/1000: loss: 0.0000000 \n",
      "\t968/1000: loss: 0.0000000 \n",
      "\t969/1000: loss: 0.0000000 \n",
      "\t970/1000: loss: 0.0000000 \n",
      "\t971/1000: loss: 0.0000000 \n",
      "\t972/1000: loss: 0.0000000 \n",
      "\t973/1000: loss: 0.0000000 \n",
      "\t974/1000: loss: 0.0000000 \n",
      "\t975/1000: loss: 0.0000000 \n",
      "\t976/1000: loss: 0.0000000 \n",
      "\t977/1000: loss: 0.0000000 \n",
      "\t978/1000: loss: 0.0000000 \n",
      "\t979/1000: loss: 0.0000000 \n",
      "\t980/1000: loss: 0.0000000 \n",
      "\t981/1000: loss: 0.0000000 \n",
      "\t982/1000: loss: 0.0000000 \n",
      "\t983/1000: loss: 0.0000000 \n",
      "\t984/1000: loss: 0.0000000 \n",
      "\t985/1000: loss: 0.0000000 \n",
      "\t986/1000: loss: 0.0000000 \n",
      "\t987/1000: loss: 0.0000000 \n",
      "\t988/1000: loss: 0.0000000 \n",
      "\t989/1000: loss: 0.0000000 \n",
      "\t990/1000: loss: 0.0000000 \n",
      "\t991/1000: loss: 0.0000000 \n",
      "\t992/1000: loss: 0.0000000 \n",
      "\t993/1000: loss: 0.0000000 \n",
      "\t994/1000: loss: 0.0000000 \n",
      "\t995/1000: loss: 0.0000000 \n",
      "\t996/1000: loss: 0.0000000 \n",
      "\t997/1000: loss: 0.0000000 \n",
      "\t998/1000: loss: 0.0000000 \n",
      "\t999/1000: loss: 0.0000000 \n",
      "\t1000/1000: loss: 0.0000000 \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(1000):\n",
    "    out = nac(X_train)\n",
    "    loss = crieterion(out, y_train)\n",
    "    losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\t{}/{}: loss: {:.7f} \".format(i+1, num_iters, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:10])\n",
    "plt.title('Losses in 10 epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 9.989236000873536e-13\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_out = nac(X_train)\n",
    "    train_loss = crieterion(train_out, y_train)\n",
    "    print(f'train loss = {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss = 9.74864611778814e-13\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_out = nac(X_test)\n",
    "    test_loss = crieterion(test_out, y_test)\n",
    "    print(f'test loss = {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Accumulator vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1/100000: mlp loss: 116.6307526 \n",
      "\t1001/100000: mlp loss: 116.6307526 \n",
      "\t2001/100000: mlp loss: 116.6307526 \n",
      "\t3001/100000: mlp loss: 116.6307526 \n",
      "\t4001/100000: mlp loss: 116.6307526 \n",
      "\t5001/100000: mlp loss: 116.6307526 \n",
      "\t6001/100000: mlp loss: 116.6307526 \n",
      "\t7001/100000: mlp loss: 116.6307526 \n",
      "\t8001/100000: mlp loss: 116.6307526 \n",
      "\t9001/100000: mlp loss: 116.6307526 \n",
      "\t10001/100000: mlp loss: 116.6307526 \n",
      "\t11001/100000: mlp loss: 116.6307526 \n",
      "\t12001/100000: mlp loss: 116.6307526 \n",
      "\t13001/100000: mlp loss: 116.6307526 \n",
      "\t14001/100000: mlp loss: 116.6307526 \n",
      "\t15001/100000: mlp loss: 116.6307526 \n",
      "\t16001/100000: mlp loss: 116.6307526 \n",
      "\t17001/100000: mlp loss: 116.6307526 \n",
      "\t18001/100000: mlp loss: 116.6307526 \n",
      "\t19001/100000: mlp loss: 116.6307526 \n",
      "\t20001/100000: mlp loss: 116.6307526 \n",
      "\t21001/100000: mlp loss: 116.6307526 \n",
      "\t22001/100000: mlp loss: 116.6307526 \n",
      "\t23001/100000: mlp loss: 116.6307526 \n",
      "\t24001/100000: mlp loss: 116.6307526 \n",
      "\t25001/100000: mlp loss: 116.6307526 \n",
      "\t26001/100000: mlp loss: 116.6307526 \n",
      "\t27001/100000: mlp loss: 116.6307526 \n",
      "\t28001/100000: mlp loss: 116.6307526 \n",
      "\t29001/100000: mlp loss: 116.6307526 \n",
      "\t30001/100000: mlp loss: 116.6307526 \n",
      "\t31001/100000: mlp loss: 116.6307526 \n",
      "\t32001/100000: mlp loss: 116.6307526 \n",
      "\t33001/100000: mlp loss: 116.6307526 \n",
      "\t34001/100000: mlp loss: 116.6307526 \n",
      "\t35001/100000: mlp loss: 116.6307526 \n",
      "\t36001/100000: mlp loss: 116.6307526 \n",
      "\t37001/100000: mlp loss: 116.6307526 \n",
      "\t38001/100000: mlp loss: 116.6307526 \n",
      "\t39001/100000: mlp loss: 116.6307526 \n",
      "\t40001/100000: mlp loss: 116.6307526 \n",
      "\t41001/100000: mlp loss: 116.6307526 \n",
      "\t42001/100000: mlp loss: 116.6307526 \n",
      "\t43001/100000: mlp loss: 116.6307526 \n",
      "\t44001/100000: mlp loss: 116.6307526 \n",
      "\t45001/100000: mlp loss: 116.6307526 \n",
      "\t46001/100000: mlp loss: 116.6307526 \n",
      "\t47001/100000: mlp loss: 116.6307526 \n",
      "\t48001/100000: mlp loss: 116.6307526 \n",
      "\t49001/100000: mlp loss: 116.6307526 \n",
      "\t50001/100000: mlp loss: 116.6307526 \n",
      "\t51001/100000: mlp loss: 116.6307526 \n",
      "\t52001/100000: mlp loss: 116.6307526 \n",
      "\t53001/100000: mlp loss: 116.6307526 \n",
      "\t54001/100000: mlp loss: 116.6307526 \n",
      "\t55001/100000: mlp loss: 116.6307526 \n",
      "\t56001/100000: mlp loss: 116.6307526 \n",
      "\t57001/100000: mlp loss: 116.6307526 \n",
      "\t58001/100000: mlp loss: 116.6307526 \n",
      "\t59001/100000: mlp loss: 116.6307526 \n",
      "\t60001/100000: mlp loss: 116.6307526 \n",
      "\t61001/100000: mlp loss: 116.6307526 \n",
      "\t62001/100000: mlp loss: 116.6307526 \n",
      "\t63001/100000: mlp loss: 116.6307526 \n",
      "\t64001/100000: mlp loss: 116.6307526 \n",
      "\t65001/100000: mlp loss: 116.6307526 \n",
      "\t66001/100000: mlp loss: 116.6307526 \n",
      "\t67001/100000: mlp loss: 116.6307526 \n",
      "\t68001/100000: mlp loss: 116.6307526 \n",
      "\t69001/100000: mlp loss: 116.6307526 \n",
      "\t70001/100000: mlp loss: 116.6307526 \n"
     ]
    }
   ],
   "source": [
    "mlp_losses = []\n",
    "num_iters = 100000\n",
    "for i in range(num_iters):\n",
    "    out = mlp(X_train)\n",
    "    loss = crieterion(out, y_train)\n",
    "    mlp_losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i% 1000 == 0:\n",
    "        print(\"\\t{}/{}: mlp loss: {:.7f} \".format(i+1, num_iters, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp_losses)\n",
    "plt.title('MLP Losses in 100000 epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Arithmetic Logic Unit Cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALU(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, num_layers, e=1e-5):\n",
    "        super(NALU, self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (dlenv)",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
