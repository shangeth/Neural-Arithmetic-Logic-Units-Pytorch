{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaper   : https://arxiv.org/pdf/1808.00508v1.pdf\\nAuthors : Andrew Trask, Felix Hill, Scott Reed, Jack Rae\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paper   : https://arxiv.org/pdf/1808.00508v1.pdf\n",
    "Authors : Andrew Trask, Felix Hill, Scott Reed, Jack Rae\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Accumulator\n",
    "\n",
    "![Neural Accumulator](https://cdn-images-1.medium.com/max/1600/1*vMYerlUvUP5gw4LDZv-aSg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumulator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NeuralAccumulator, self).__init__()\n",
    "        self.W1 = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W2 = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.register_parameter('bias', None)\n",
    "        \n",
    "        self.W = Parameter(torch.tanh(self.W1) * torch.sigmoid(self.W2))\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = nn.functional.linear(x, self.W, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Accumulator for Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(train_size, test_size, fn):\n",
    "    X = torch.Tensor(train_size + test_size, 2)\n",
    "    Y = torch.Tensor(train_size + test_size, 1)\n",
    "\n",
    "    for i in range(train_size + test_size):\n",
    "        x = torch.rand(2)*10\n",
    "        y = torch.tensor([fn(*x)])\n",
    "        X[i] = x\n",
    "        Y[i] = y\n",
    "    \n",
    "    X_train, y_train = X[:train_size], Y[:train_size]\n",
    "    X_test, y_test = X[train_size:], Y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: x + y\n",
    "X_train, y_train, X_test, y_test = dataset(1000, 200, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([ 5.6294,  3.7205])\ty = tensor([ 9.3499])\n",
      "X = tensor([ 3.3979,  0.0743])\ty = tensor([ 3.4722])\n",
      "X = tensor([ 2.2337,  0.3268])\ty = tensor([ 2.5605])\n",
      "X = tensor([ 4.2039,  9.6030])\ty = tensor([ 13.8069])\n",
      "X = tensor([ 1.9430,  1.5801])\ty = tensor([ 3.5230])\n",
      "X = tensor([ 7.1482,  3.1841])\ty = tensor([ 10.3323])\n",
      "X = tensor([ 7.6909,  8.4041])\ty = tensor([ 16.0950])\n",
      "X = tensor([ 5.4284,  2.1688])\ty = tensor([ 7.5972])\n",
      "X = tensor([ 3.5210,  7.8779])\ty = tensor([ 11.3989])\n",
      "X = tensor([ 5.3086,  8.7236])\ty = tensor([ 14.0322])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'X = {X_train[i]}\\ty = {y_train[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nac = NeuralAccumulator(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.8147])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nac(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(nac.parameters(), lr=1e-2)\n",
    "crieterion = criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1/1000: loss: 66.5945282 \n",
      "\t2/1000: loss: 2.5424151 \n",
      "\t3/1000: loss: 0.5567204 \n",
      "\t4/1000: loss: 0.3531415 \n",
      "\t5/1000: loss: 0.2442876 \n",
      "\t6/1000: loss: 0.1696015 \n",
      "\t7/1000: loss: 0.1177661 \n",
      "\t8/1000: loss: 0.0817736 \n",
      "\t9/1000: loss: 0.0567815 \n",
      "\t10/1000: loss: 0.0394277 \n",
      "\t11/1000: loss: 0.0273775 \n",
      "\t12/1000: loss: 0.0190103 \n",
      "\t13/1000: loss: 0.0132002 \n",
      "\t14/1000: loss: 0.0091659 \n",
      "\t15/1000: loss: 0.0063645 \n",
      "\t16/1000: loss: 0.0044194 \n",
      "\t17/1000: loss: 0.0030687 \n",
      "\t18/1000: loss: 0.0021308 \n",
      "\t19/1000: loss: 0.0014796 \n",
      "\t20/1000: loss: 0.0010274 \n",
      "\t21/1000: loss: 0.0007134 \n",
      "\t22/1000: loss: 0.0004954 \n",
      "\t23/1000: loss: 0.0003440 \n",
      "\t24/1000: loss: 0.0002388 \n",
      "\t25/1000: loss: 0.0001658 \n",
      "\t26/1000: loss: 0.0001152 \n",
      "\t27/1000: loss: 0.0000800 \n",
      "\t28/1000: loss: 0.0000555 \n",
      "\t29/1000: loss: 0.0000386 \n",
      "\t30/1000: loss: 0.0000268 \n",
      "\t31/1000: loss: 0.0000186 \n",
      "\t32/1000: loss: 0.0000129 \n",
      "\t33/1000: loss: 0.0000090 \n",
      "\t34/1000: loss: 0.0000062 \n",
      "\t35/1000: loss: 0.0000043 \n",
      "\t36/1000: loss: 0.0000030 \n",
      "\t37/1000: loss: 0.0000021 \n",
      "\t38/1000: loss: 0.0000014 \n",
      "\t39/1000: loss: 0.0000010 \n",
      "\t40/1000: loss: 0.0000007 \n",
      "\t41/1000: loss: 0.0000005 \n",
      "\t42/1000: loss: 0.0000003 \n",
      "\t43/1000: loss: 0.0000002 \n",
      "\t44/1000: loss: 0.0000002 \n",
      "\t45/1000: loss: 0.0000001 \n",
      "\t46/1000: loss: 0.0000001 \n",
      "\t47/1000: loss: 0.0000001 \n",
      "\t48/1000: loss: 0.0000000 \n",
      "\t49/1000: loss: 0.0000000 \n",
      "\t50/1000: loss: 0.0000000 \n",
      "\t51/1000: loss: 0.0000000 \n",
      "\t52/1000: loss: 0.0000000 \n",
      "\t53/1000: loss: 0.0000000 \n",
      "\t54/1000: loss: 0.0000000 \n",
      "\t55/1000: loss: 0.0000000 \n",
      "\t56/1000: loss: 0.0000000 \n",
      "\t57/1000: loss: 0.0000000 \n",
      "\t58/1000: loss: 0.0000000 \n",
      "\t59/1000: loss: 0.0000000 \n",
      "\t60/1000: loss: 0.0000000 \n",
      "\t61/1000: loss: 0.0000000 \n",
      "\t62/1000: loss: 0.0000000 \n",
      "\t63/1000: loss: 0.0000000 \n",
      "\t64/1000: loss: 0.0000000 \n",
      "\t65/1000: loss: 0.0000000 \n",
      "\t66/1000: loss: 0.0000000 \n",
      "\t67/1000: loss: 0.0000000 \n",
      "\t68/1000: loss: 0.0000000 \n",
      "\t69/1000: loss: 0.0000000 \n",
      "\t70/1000: loss: 0.0000000 \n",
      "\t71/1000: loss: 0.0000000 \n",
      "\t72/1000: loss: 0.0000000 \n",
      "\t73/1000: loss: 0.0000000 \n",
      "\t74/1000: loss: 0.0000000 \n",
      "\t75/1000: loss: 0.0000000 \n",
      "\t76/1000: loss: 0.0000000 \n",
      "\t77/1000: loss: 0.0000000 \n",
      "\t78/1000: loss: 0.0000000 \n",
      "\t79/1000: loss: 0.0000000 \n",
      "\t80/1000: loss: 0.0000000 \n",
      "\t81/1000: loss: 0.0000000 \n",
      "\t82/1000: loss: 0.0000000 \n",
      "\t83/1000: loss: 0.0000000 \n",
      "\t84/1000: loss: 0.0000000 \n",
      "\t85/1000: loss: 0.0000000 \n",
      "\t86/1000: loss: 0.0000000 \n",
      "\t87/1000: loss: 0.0000000 \n",
      "\t88/1000: loss: 0.0000000 \n",
      "\t89/1000: loss: 0.0000000 \n",
      "\t90/1000: loss: 0.0000000 \n",
      "\t91/1000: loss: 0.0000000 \n",
      "\t92/1000: loss: 0.0000000 \n",
      "\t93/1000: loss: 0.0000000 \n",
      "\t94/1000: loss: 0.0000000 \n",
      "\t95/1000: loss: 0.0000000 \n",
      "\t96/1000: loss: 0.0000000 \n",
      "\t97/1000: loss: 0.0000000 \n",
      "\t98/1000: loss: 0.0000000 \n",
      "\t99/1000: loss: 0.0000000 \n",
      "\t100/1000: loss: 0.0000000 \n",
      "\t101/1000: loss: 0.0000000 \n",
      "\t102/1000: loss: 0.0000000 \n",
      "\t103/1000: loss: 0.0000000 \n",
      "\t104/1000: loss: 0.0000000 \n",
      "\t105/1000: loss: 0.0000000 \n",
      "\t106/1000: loss: 0.0000000 \n",
      "\t107/1000: loss: 0.0000000 \n",
      "\t108/1000: loss: 0.0000000 \n",
      "\t109/1000: loss: 0.0000000 \n",
      "\t110/1000: loss: 0.0000000 \n",
      "\t111/1000: loss: 0.0000000 \n",
      "\t112/1000: loss: 0.0000000 \n",
      "\t113/1000: loss: 0.0000000 \n",
      "\t114/1000: loss: 0.0000000 \n",
      "\t115/1000: loss: 0.0000000 \n",
      "\t116/1000: loss: 0.0000000 \n",
      "\t117/1000: loss: 0.0000000 \n",
      "\t118/1000: loss: 0.0000000 \n",
      "\t119/1000: loss: 0.0000000 \n",
      "\t120/1000: loss: 0.0000000 \n",
      "\t121/1000: loss: 0.0000000 \n",
      "\t122/1000: loss: 0.0000000 \n",
      "\t123/1000: loss: 0.0000000 \n",
      "\t124/1000: loss: 0.0000000 \n",
      "\t125/1000: loss: 0.0000000 \n",
      "\t126/1000: loss: 0.0000000 \n",
      "\t127/1000: loss: 0.0000000 \n",
      "\t128/1000: loss: 0.0000000 \n",
      "\t129/1000: loss: 0.0000000 \n",
      "\t130/1000: loss: 0.0000000 \n",
      "\t131/1000: loss: 0.0000000 \n",
      "\t132/1000: loss: 0.0000000 \n",
      "\t133/1000: loss: 0.0000000 \n",
      "\t134/1000: loss: 0.0000000 \n",
      "\t135/1000: loss: 0.0000000 \n",
      "\t136/1000: loss: 0.0000000 \n",
      "\t137/1000: loss: 0.0000000 \n",
      "\t138/1000: loss: 0.0000000 \n",
      "\t139/1000: loss: 0.0000000 \n",
      "\t140/1000: loss: 0.0000000 \n",
      "\t141/1000: loss: 0.0000000 \n",
      "\t142/1000: loss: 0.0000000 \n",
      "\t143/1000: loss: 0.0000000 \n",
      "\t144/1000: loss: 0.0000000 \n",
      "\t145/1000: loss: 0.0000000 \n",
      "\t146/1000: loss: 0.0000000 \n",
      "\t147/1000: loss: 0.0000000 \n",
      "\t148/1000: loss: 0.0000000 \n",
      "\t149/1000: loss: 0.0000000 \n",
      "\t150/1000: loss: 0.0000000 \n",
      "\t151/1000: loss: 0.0000000 \n",
      "\t152/1000: loss: 0.0000000 \n",
      "\t153/1000: loss: 0.0000000 \n",
      "\t154/1000: loss: 0.0000000 \n",
      "\t155/1000: loss: 0.0000000 \n",
      "\t156/1000: loss: 0.0000000 \n",
      "\t157/1000: loss: 0.0000000 \n",
      "\t158/1000: loss: 0.0000000 \n",
      "\t159/1000: loss: 0.0000000 \n",
      "\t160/1000: loss: 0.0000000 \n",
      "\t161/1000: loss: 0.0000000 \n",
      "\t162/1000: loss: 0.0000000 \n",
      "\t163/1000: loss: 0.0000000 \n",
      "\t164/1000: loss: 0.0000000 \n",
      "\t165/1000: loss: 0.0000000 \n",
      "\t166/1000: loss: 0.0000000 \n",
      "\t167/1000: loss: 0.0000000 \n",
      "\t168/1000: loss: 0.0000000 \n",
      "\t169/1000: loss: 0.0000000 \n",
      "\t170/1000: loss: 0.0000000 \n",
      "\t171/1000: loss: 0.0000000 \n",
      "\t172/1000: loss: 0.0000000 \n",
      "\t173/1000: loss: 0.0000000 \n",
      "\t174/1000: loss: 0.0000000 \n",
      "\t175/1000: loss: 0.0000000 \n",
      "\t176/1000: loss: 0.0000000 \n",
      "\t177/1000: loss: 0.0000000 \n",
      "\t178/1000: loss: 0.0000000 \n",
      "\t179/1000: loss: 0.0000000 \n",
      "\t180/1000: loss: 0.0000000 \n",
      "\t181/1000: loss: 0.0000000 \n",
      "\t182/1000: loss: 0.0000000 \n",
      "\t183/1000: loss: 0.0000000 \n",
      "\t184/1000: loss: 0.0000000 \n",
      "\t185/1000: loss: 0.0000000 \n",
      "\t186/1000: loss: 0.0000000 \n",
      "\t187/1000: loss: 0.0000000 \n",
      "\t188/1000: loss: 0.0000000 \n",
      "\t189/1000: loss: 0.0000000 \n",
      "\t190/1000: loss: 0.0000000 \n",
      "\t191/1000: loss: 0.0000000 \n",
      "\t192/1000: loss: 0.0000000 \n",
      "\t193/1000: loss: 0.0000000 \n",
      "\t194/1000: loss: 0.0000000 \n",
      "\t195/1000: loss: 0.0000000 \n",
      "\t196/1000: loss: 0.0000000 \n",
      "\t197/1000: loss: 0.0000000 \n",
      "\t198/1000: loss: 0.0000000 \n",
      "\t199/1000: loss: 0.0000000 \n",
      "\t200/1000: loss: 0.0000000 \n",
      "\t201/1000: loss: 0.0000000 \n",
      "\t202/1000: loss: 0.0000000 \n",
      "\t203/1000: loss: 0.0000000 \n",
      "\t204/1000: loss: 0.0000000 \n",
      "\t205/1000: loss: 0.0000000 \n",
      "\t206/1000: loss: 0.0000000 \n",
      "\t207/1000: loss: 0.0000000 \n",
      "\t208/1000: loss: 0.0000000 \n",
      "\t209/1000: loss: 0.0000000 \n",
      "\t210/1000: loss: 0.0000000 \n",
      "\t211/1000: loss: 0.0000000 \n",
      "\t212/1000: loss: 0.0000000 \n",
      "\t213/1000: loss: 0.0000000 \n",
      "\t214/1000: loss: 0.0000000 \n",
      "\t215/1000: loss: 0.0000000 \n",
      "\t216/1000: loss: 0.0000000 \n",
      "\t217/1000: loss: 0.0000000 \n",
      "\t218/1000: loss: 0.0000000 \n",
      "\t219/1000: loss: 0.0000000 \n",
      "\t220/1000: loss: 0.0000000 \n",
      "\t221/1000: loss: 0.0000000 \n",
      "\t222/1000: loss: 0.0000000 \n",
      "\t223/1000: loss: 0.0000000 \n",
      "\t224/1000: loss: 0.0000000 \n",
      "\t225/1000: loss: 0.0000000 \n",
      "\t226/1000: loss: 0.0000000 \n",
      "\t227/1000: loss: 0.0000000 \n",
      "\t228/1000: loss: 0.0000000 \n",
      "\t229/1000: loss: 0.0000000 \n",
      "\t230/1000: loss: 0.0000000 \n",
      "\t231/1000: loss: 0.0000000 \n",
      "\t232/1000: loss: 0.0000000 \n",
      "\t233/1000: loss: 0.0000000 \n",
      "\t234/1000: loss: 0.0000000 \n",
      "\t235/1000: loss: 0.0000000 \n",
      "\t236/1000: loss: 0.0000000 \n",
      "\t237/1000: loss: 0.0000000 \n",
      "\t238/1000: loss: 0.0000000 \n",
      "\t239/1000: loss: 0.0000000 \n",
      "\t240/1000: loss: 0.0000000 \n",
      "\t241/1000: loss: 0.0000000 \n",
      "\t242/1000: loss: 0.0000000 \n",
      "\t243/1000: loss: 0.0000000 \n",
      "\t244/1000: loss: 0.0000000 \n",
      "\t245/1000: loss: 0.0000000 \n",
      "\t246/1000: loss: 0.0000000 \n",
      "\t247/1000: loss: 0.0000000 \n",
      "\t248/1000: loss: 0.0000000 \n",
      "\t249/1000: loss: 0.0000000 \n",
      "\t250/1000: loss: 0.0000000 \n",
      "\t251/1000: loss: 0.0000000 \n",
      "\t252/1000: loss: 0.0000000 \n",
      "\t253/1000: loss: 0.0000000 \n",
      "\t254/1000: loss: 0.0000000 \n",
      "\t255/1000: loss: 0.0000000 \n",
      "\t256/1000: loss: 0.0000000 \n",
      "\t257/1000: loss: 0.0000000 \n",
      "\t258/1000: loss: 0.0000000 \n",
      "\t259/1000: loss: 0.0000000 \n",
      "\t260/1000: loss: 0.0000000 \n",
      "\t261/1000: loss: 0.0000000 \n",
      "\t262/1000: loss: 0.0000000 \n",
      "\t263/1000: loss: 0.0000000 \n",
      "\t264/1000: loss: 0.0000000 \n",
      "\t265/1000: loss: 0.0000000 \n",
      "\t266/1000: loss: 0.0000000 \n",
      "\t267/1000: loss: 0.0000000 \n",
      "\t268/1000: loss: 0.0000000 \n",
      "\t269/1000: loss: 0.0000000 \n",
      "\t270/1000: loss: 0.0000000 \n",
      "\t271/1000: loss: 0.0000000 \n",
      "\t272/1000: loss: 0.0000000 \n",
      "\t273/1000: loss: 0.0000000 \n",
      "\t274/1000: loss: 0.0000000 \n",
      "\t275/1000: loss: 0.0000000 \n",
      "\t276/1000: loss: 0.0000000 \n",
      "\t277/1000: loss: 0.0000000 \n",
      "\t278/1000: loss: 0.0000000 \n",
      "\t279/1000: loss: 0.0000000 \n",
      "\t280/1000: loss: 0.0000000 \n",
      "\t281/1000: loss: 0.0000000 \n",
      "\t282/1000: loss: 0.0000000 \n",
      "\t283/1000: loss: 0.0000000 \n",
      "\t284/1000: loss: 0.0000000 \n",
      "\t285/1000: loss: 0.0000000 \n",
      "\t286/1000: loss: 0.0000000 \n",
      "\t287/1000: loss: 0.0000000 \n",
      "\t288/1000: loss: 0.0000000 \n",
      "\t289/1000: loss: 0.0000000 \n",
      "\t290/1000: loss: 0.0000000 \n",
      "\t291/1000: loss: 0.0000000 \n",
      "\t292/1000: loss: 0.0000000 \n",
      "\t293/1000: loss: 0.0000000 \n",
      "\t294/1000: loss: 0.0000000 \n",
      "\t295/1000: loss: 0.0000000 \n",
      "\t296/1000: loss: 0.0000000 \n",
      "\t297/1000: loss: 0.0000000 \n",
      "\t298/1000: loss: 0.0000000 \n",
      "\t299/1000: loss: 0.0000000 \n",
      "\t300/1000: loss: 0.0000000 \n",
      "\t301/1000: loss: 0.0000000 \n",
      "\t302/1000: loss: 0.0000000 \n",
      "\t303/1000: loss: 0.0000000 \n",
      "\t304/1000: loss: 0.0000000 \n",
      "\t305/1000: loss: 0.0000000 \n",
      "\t306/1000: loss: 0.0000000 \n",
      "\t307/1000: loss: 0.0000000 \n",
      "\t308/1000: loss: 0.0000000 \n",
      "\t309/1000: loss: 0.0000000 \n",
      "\t310/1000: loss: 0.0000000 \n",
      "\t311/1000: loss: 0.0000000 \n",
      "\t312/1000: loss: 0.0000000 \n",
      "\t313/1000: loss: 0.0000000 \n",
      "\t314/1000: loss: 0.0000000 \n",
      "\t315/1000: loss: 0.0000000 \n",
      "\t316/1000: loss: 0.0000000 \n",
      "\t317/1000: loss: 0.0000000 \n",
      "\t318/1000: loss: 0.0000000 \n",
      "\t319/1000: loss: 0.0000000 \n",
      "\t320/1000: loss: 0.0000000 \n",
      "\t321/1000: loss: 0.0000000 \n",
      "\t322/1000: loss: 0.0000000 \n",
      "\t323/1000: loss: 0.0000000 \n",
      "\t324/1000: loss: 0.0000000 \n",
      "\t325/1000: loss: 0.0000000 \n",
      "\t326/1000: loss: 0.0000000 \n",
      "\t327/1000: loss: 0.0000000 \n",
      "\t328/1000: loss: 0.0000000 \n",
      "\t329/1000: loss: 0.0000000 \n",
      "\t330/1000: loss: 0.0000000 \n",
      "\t331/1000: loss: 0.0000000 \n",
      "\t332/1000: loss: 0.0000000 \n",
      "\t333/1000: loss: 0.0000000 \n",
      "\t334/1000: loss: 0.0000000 \n",
      "\t335/1000: loss: 0.0000000 \n",
      "\t336/1000: loss: 0.0000000 \n",
      "\t337/1000: loss: 0.0000000 \n",
      "\t338/1000: loss: 0.0000000 \n",
      "\t339/1000: loss: 0.0000000 \n",
      "\t340/1000: loss: 0.0000000 \n",
      "\t341/1000: loss: 0.0000000 \n",
      "\t342/1000: loss: 0.0000000 \n",
      "\t343/1000: loss: 0.0000000 \n",
      "\t344/1000: loss: 0.0000000 \n",
      "\t345/1000: loss: 0.0000000 \n",
      "\t346/1000: loss: 0.0000000 \n",
      "\t347/1000: loss: 0.0000000 \n",
      "\t348/1000: loss: 0.0000000 \n",
      "\t349/1000: loss: 0.0000000 \n",
      "\t350/1000: loss: 0.0000000 \n",
      "\t351/1000: loss: 0.0000000 \n",
      "\t352/1000: loss: 0.0000000 \n",
      "\t353/1000: loss: 0.0000000 \n",
      "\t354/1000: loss: 0.0000000 \n",
      "\t355/1000: loss: 0.0000000 \n",
      "\t356/1000: loss: 0.0000000 \n",
      "\t357/1000: loss: 0.0000000 \n",
      "\t358/1000: loss: 0.0000000 \n",
      "\t359/1000: loss: 0.0000000 \n",
      "\t360/1000: loss: 0.0000000 \n",
      "\t361/1000: loss: 0.0000000 \n",
      "\t362/1000: loss: 0.0000000 \n",
      "\t363/1000: loss: 0.0000000 \n",
      "\t364/1000: loss: 0.0000000 \n",
      "\t365/1000: loss: 0.0000000 \n",
      "\t366/1000: loss: 0.0000000 \n",
      "\t367/1000: loss: 0.0000000 \n",
      "\t368/1000: loss: 0.0000000 \n",
      "\t369/1000: loss: 0.0000000 \n",
      "\t370/1000: loss: 0.0000000 \n",
      "\t371/1000: loss: 0.0000000 \n",
      "\t372/1000: loss: 0.0000000 \n",
      "\t373/1000: loss: 0.0000000 \n",
      "\t374/1000: loss: 0.0000000 \n",
      "\t375/1000: loss: 0.0000000 \n",
      "\t376/1000: loss: 0.0000000 \n",
      "\t377/1000: loss: 0.0000000 \n",
      "\t378/1000: loss: 0.0000000 \n",
      "\t379/1000: loss: 0.0000000 \n",
      "\t380/1000: loss: 0.0000000 \n",
      "\t381/1000: loss: 0.0000000 \n",
      "\t382/1000: loss: 0.0000000 \n",
      "\t383/1000: loss: 0.0000000 \n",
      "\t384/1000: loss: 0.0000000 \n",
      "\t385/1000: loss: 0.0000000 \n",
      "\t386/1000: loss: 0.0000000 \n",
      "\t387/1000: loss: 0.0000000 \n",
      "\t388/1000: loss: 0.0000000 \n",
      "\t389/1000: loss: 0.0000000 \n",
      "\t390/1000: loss: 0.0000000 \n",
      "\t391/1000: loss: 0.0000000 \n",
      "\t392/1000: loss: 0.0000000 \n",
      "\t393/1000: loss: 0.0000000 \n",
      "\t394/1000: loss: 0.0000000 \n",
      "\t395/1000: loss: 0.0000000 \n",
      "\t396/1000: loss: 0.0000000 \n",
      "\t397/1000: loss: 0.0000000 \n",
      "\t398/1000: loss: 0.0000000 \n",
      "\t399/1000: loss: 0.0000000 \n",
      "\t400/1000: loss: 0.0000000 \n",
      "\t401/1000: loss: 0.0000000 \n",
      "\t402/1000: loss: 0.0000000 \n",
      "\t403/1000: loss: 0.0000000 \n",
      "\t404/1000: loss: 0.0000000 \n",
      "\t405/1000: loss: 0.0000000 \n",
      "\t406/1000: loss: 0.0000000 \n",
      "\t407/1000: loss: 0.0000000 \n",
      "\t408/1000: loss: 0.0000000 \n",
      "\t409/1000: loss: 0.0000000 \n",
      "\t410/1000: loss: 0.0000000 \n",
      "\t411/1000: loss: 0.0000000 \n",
      "\t412/1000: loss: 0.0000000 \n",
      "\t413/1000: loss: 0.0000000 \n",
      "\t414/1000: loss: 0.0000000 \n",
      "\t415/1000: loss: 0.0000000 \n",
      "\t416/1000: loss: 0.0000000 \n",
      "\t417/1000: loss: 0.0000000 \n",
      "\t418/1000: loss: 0.0000000 \n",
      "\t419/1000: loss: 0.0000000 \n",
      "\t420/1000: loss: 0.0000000 \n",
      "\t421/1000: loss: 0.0000000 \n",
      "\t422/1000: loss: 0.0000000 \n",
      "\t423/1000: loss: 0.0000000 \n",
      "\t424/1000: loss: 0.0000000 \n",
      "\t425/1000: loss: 0.0000000 \n",
      "\t426/1000: loss: 0.0000000 \n",
      "\t427/1000: loss: 0.0000000 \n",
      "\t428/1000: loss: 0.0000000 \n",
      "\t429/1000: loss: 0.0000000 \n",
      "\t430/1000: loss: 0.0000000 \n",
      "\t431/1000: loss: 0.0000000 \n",
      "\t432/1000: loss: 0.0000000 \n",
      "\t433/1000: loss: 0.0000000 \n",
      "\t434/1000: loss: 0.0000000 \n",
      "\t435/1000: loss: 0.0000000 \n",
      "\t436/1000: loss: 0.0000000 \n",
      "\t437/1000: loss: 0.0000000 \n",
      "\t438/1000: loss: 0.0000000 \n",
      "\t439/1000: loss: 0.0000000 \n",
      "\t440/1000: loss: 0.0000000 \n",
      "\t441/1000: loss: 0.0000000 \n",
      "\t442/1000: loss: 0.0000000 \n",
      "\t443/1000: loss: 0.0000000 \n",
      "\t444/1000: loss: 0.0000000 \n",
      "\t445/1000: loss: 0.0000000 \n",
      "\t446/1000: loss: 0.0000000 \n",
      "\t447/1000: loss: 0.0000000 \n",
      "\t448/1000: loss: 0.0000000 \n",
      "\t449/1000: loss: 0.0000000 \n",
      "\t450/1000: loss: 0.0000000 \n",
      "\t451/1000: loss: 0.0000000 \n",
      "\t452/1000: loss: 0.0000000 \n",
      "\t453/1000: loss: 0.0000000 \n",
      "\t454/1000: loss: 0.0000000 \n",
      "\t455/1000: loss: 0.0000000 \n",
      "\t456/1000: loss: 0.0000000 \n",
      "\t457/1000: loss: 0.0000000 \n",
      "\t458/1000: loss: 0.0000000 \n",
      "\t459/1000: loss: 0.0000000 \n",
      "\t460/1000: loss: 0.0000000 \n",
      "\t461/1000: loss: 0.0000000 \n",
      "\t462/1000: loss: 0.0000000 \n",
      "\t463/1000: loss: 0.0000000 \n",
      "\t464/1000: loss: 0.0000000 \n",
      "\t465/1000: loss: 0.0000000 \n",
      "\t466/1000: loss: 0.0000000 \n",
      "\t467/1000: loss: 0.0000000 \n",
      "\t468/1000: loss: 0.0000000 \n",
      "\t469/1000: loss: 0.0000000 \n",
      "\t470/1000: loss: 0.0000000 \n",
      "\t471/1000: loss: 0.0000000 \n",
      "\t472/1000: loss: 0.0000000 \n",
      "\t473/1000: loss: 0.0000000 \n",
      "\t474/1000: loss: 0.0000000 \n",
      "\t475/1000: loss: 0.0000000 \n",
      "\t476/1000: loss: 0.0000000 \n",
      "\t477/1000: loss: 0.0000000 \n",
      "\t478/1000: loss: 0.0000000 \n",
      "\t479/1000: loss: 0.0000000 \n",
      "\t480/1000: loss: 0.0000000 \n",
      "\t481/1000: loss: 0.0000000 \n",
      "\t482/1000: loss: 0.0000000 \n",
      "\t483/1000: loss: 0.0000000 \n",
      "\t484/1000: loss: 0.0000000 \n",
      "\t485/1000: loss: 0.0000000 \n",
      "\t486/1000: loss: 0.0000000 \n",
      "\t487/1000: loss: 0.0000000 \n",
      "\t488/1000: loss: 0.0000000 \n",
      "\t489/1000: loss: 0.0000000 \n",
      "\t490/1000: loss: 0.0000000 \n",
      "\t491/1000: loss: 0.0000000 \n",
      "\t492/1000: loss: 0.0000000 \n",
      "\t493/1000: loss: 0.0000000 \n",
      "\t494/1000: loss: 0.0000000 \n",
      "\t495/1000: loss: 0.0000000 \n",
      "\t496/1000: loss: 0.0000000 \n",
      "\t497/1000: loss: 0.0000000 \n",
      "\t498/1000: loss: 0.0000000 \n",
      "\t499/1000: loss: 0.0000000 \n",
      "\t500/1000: loss: 0.0000000 \n",
      "\t501/1000: loss: 0.0000000 \n",
      "\t502/1000: loss: 0.0000000 \n",
      "\t503/1000: loss: 0.0000000 \n",
      "\t504/1000: loss: 0.0000000 \n",
      "\t505/1000: loss: 0.0000000 \n",
      "\t506/1000: loss: 0.0000000 \n",
      "\t507/1000: loss: 0.0000000 \n",
      "\t508/1000: loss: 0.0000000 \n",
      "\t509/1000: loss: 0.0000000 \n",
      "\t510/1000: loss: 0.0000000 \n",
      "\t511/1000: loss: 0.0000000 \n",
      "\t512/1000: loss: 0.0000000 \n",
      "\t513/1000: loss: 0.0000000 \n",
      "\t514/1000: loss: 0.0000000 \n",
      "\t515/1000: loss: 0.0000000 \n",
      "\t516/1000: loss: 0.0000000 \n",
      "\t517/1000: loss: 0.0000000 \n",
      "\t518/1000: loss: 0.0000000 \n",
      "\t519/1000: loss: 0.0000000 \n",
      "\t520/1000: loss: 0.0000000 \n",
      "\t521/1000: loss: 0.0000000 \n",
      "\t522/1000: loss: 0.0000000 \n",
      "\t523/1000: loss: 0.0000000 \n",
      "\t524/1000: loss: 0.0000000 \n",
      "\t525/1000: loss: 0.0000000 \n",
      "\t526/1000: loss: 0.0000000 \n",
      "\t527/1000: loss: 0.0000000 \n",
      "\t528/1000: loss: 0.0000000 \n",
      "\t529/1000: loss: 0.0000000 \n",
      "\t530/1000: loss: 0.0000000 \n",
      "\t531/1000: loss: 0.0000000 \n",
      "\t532/1000: loss: 0.0000000 \n",
      "\t533/1000: loss: 0.0000000 \n",
      "\t534/1000: loss: 0.0000000 \n",
      "\t535/1000: loss: 0.0000000 \n",
      "\t536/1000: loss: 0.0000000 \n",
      "\t537/1000: loss: 0.0000000 \n",
      "\t538/1000: loss: 0.0000000 \n",
      "\t539/1000: loss: 0.0000000 \n",
      "\t540/1000: loss: 0.0000000 \n",
      "\t541/1000: loss: 0.0000000 \n",
      "\t542/1000: loss: 0.0000000 \n",
      "\t543/1000: loss: 0.0000000 \n",
      "\t544/1000: loss: 0.0000000 \n",
      "\t545/1000: loss: 0.0000000 \n",
      "\t546/1000: loss: 0.0000000 \n",
      "\t547/1000: loss: 0.0000000 \n",
      "\t548/1000: loss: 0.0000000 \n",
      "\t549/1000: loss: 0.0000000 \n",
      "\t550/1000: loss: 0.0000000 \n",
      "\t551/1000: loss: 0.0000000 \n",
      "\t552/1000: loss: 0.0000000 \n",
      "\t553/1000: loss: 0.0000000 \n",
      "\t554/1000: loss: 0.0000000 \n",
      "\t555/1000: loss: 0.0000000 \n",
      "\t556/1000: loss: 0.0000000 \n",
      "\t557/1000: loss: 0.0000000 \n",
      "\t558/1000: loss: 0.0000000 \n",
      "\t559/1000: loss: 0.0000000 \n",
      "\t560/1000: loss: 0.0000000 \n",
      "\t561/1000: loss: 0.0000000 \n",
      "\t562/1000: loss: 0.0000000 \n",
      "\t563/1000: loss: 0.0000000 \n",
      "\t564/1000: loss: 0.0000000 \n",
      "\t565/1000: loss: 0.0000000 \n",
      "\t566/1000: loss: 0.0000000 \n",
      "\t567/1000: loss: 0.0000000 \n",
      "\t568/1000: loss: 0.0000000 \n",
      "\t569/1000: loss: 0.0000000 \n",
      "\t570/1000: loss: 0.0000000 \n",
      "\t571/1000: loss: 0.0000000 \n",
      "\t572/1000: loss: 0.0000000 \n",
      "\t573/1000: loss: 0.0000000 \n",
      "\t574/1000: loss: 0.0000000 \n",
      "\t575/1000: loss: 0.0000000 \n",
      "\t576/1000: loss: 0.0000000 \n",
      "\t577/1000: loss: 0.0000000 \n",
      "\t578/1000: loss: 0.0000000 \n",
      "\t579/1000: loss: 0.0000000 \n",
      "\t580/1000: loss: 0.0000000 \n",
      "\t581/1000: loss: 0.0000000 \n",
      "\t582/1000: loss: 0.0000000 \n",
      "\t583/1000: loss: 0.0000000 \n",
      "\t584/1000: loss: 0.0000000 \n",
      "\t585/1000: loss: 0.0000000 \n",
      "\t586/1000: loss: 0.0000000 \n",
      "\t587/1000: loss: 0.0000000 \n",
      "\t588/1000: loss: 0.0000000 \n",
      "\t589/1000: loss: 0.0000000 \n",
      "\t590/1000: loss: 0.0000000 \n",
      "\t591/1000: loss: 0.0000000 \n",
      "\t592/1000: loss: 0.0000000 \n",
      "\t593/1000: loss: 0.0000000 \n",
      "\t594/1000: loss: 0.0000000 \n",
      "\t595/1000: loss: 0.0000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t596/1000: loss: 0.0000000 \n",
      "\t597/1000: loss: 0.0000000 \n",
      "\t598/1000: loss: 0.0000000 \n",
      "\t599/1000: loss: 0.0000000 \n",
      "\t600/1000: loss: 0.0000000 \n",
      "\t601/1000: loss: 0.0000000 \n",
      "\t602/1000: loss: 0.0000000 \n",
      "\t603/1000: loss: 0.0000000 \n",
      "\t604/1000: loss: 0.0000000 \n",
      "\t605/1000: loss: 0.0000000 \n",
      "\t606/1000: loss: 0.0000000 \n",
      "\t607/1000: loss: 0.0000000 \n",
      "\t608/1000: loss: 0.0000000 \n",
      "\t609/1000: loss: 0.0000000 \n",
      "\t610/1000: loss: 0.0000000 \n",
      "\t611/1000: loss: 0.0000000 \n",
      "\t612/1000: loss: 0.0000000 \n",
      "\t613/1000: loss: 0.0000000 \n",
      "\t614/1000: loss: 0.0000000 \n",
      "\t615/1000: loss: 0.0000000 \n",
      "\t616/1000: loss: 0.0000000 \n",
      "\t617/1000: loss: 0.0000000 \n",
      "\t618/1000: loss: 0.0000000 \n",
      "\t619/1000: loss: 0.0000000 \n",
      "\t620/1000: loss: 0.0000000 \n",
      "\t621/1000: loss: 0.0000000 \n",
      "\t622/1000: loss: 0.0000000 \n",
      "\t623/1000: loss: 0.0000000 \n",
      "\t624/1000: loss: 0.0000000 \n",
      "\t625/1000: loss: 0.0000000 \n",
      "\t626/1000: loss: 0.0000000 \n",
      "\t627/1000: loss: 0.0000000 \n",
      "\t628/1000: loss: 0.0000000 \n",
      "\t629/1000: loss: 0.0000000 \n",
      "\t630/1000: loss: 0.0000000 \n",
      "\t631/1000: loss: 0.0000000 \n",
      "\t632/1000: loss: 0.0000000 \n",
      "\t633/1000: loss: 0.0000000 \n",
      "\t634/1000: loss: 0.0000000 \n",
      "\t635/1000: loss: 0.0000000 \n",
      "\t636/1000: loss: 0.0000000 \n",
      "\t637/1000: loss: 0.0000000 \n",
      "\t638/1000: loss: 0.0000000 \n",
      "\t639/1000: loss: 0.0000000 \n",
      "\t640/1000: loss: 0.0000000 \n",
      "\t641/1000: loss: 0.0000000 \n",
      "\t642/1000: loss: 0.0000000 \n",
      "\t643/1000: loss: 0.0000000 \n",
      "\t644/1000: loss: 0.0000000 \n",
      "\t645/1000: loss: 0.0000000 \n",
      "\t646/1000: loss: 0.0000000 \n",
      "\t647/1000: loss: 0.0000000 \n",
      "\t648/1000: loss: 0.0000000 \n",
      "\t649/1000: loss: 0.0000000 \n",
      "\t650/1000: loss: 0.0000000 \n",
      "\t651/1000: loss: 0.0000000 \n",
      "\t652/1000: loss: 0.0000000 \n",
      "\t653/1000: loss: 0.0000000 \n",
      "\t654/1000: loss: 0.0000000 \n",
      "\t655/1000: loss: 0.0000000 \n",
      "\t656/1000: loss: 0.0000000 \n",
      "\t657/1000: loss: 0.0000000 \n",
      "\t658/1000: loss: 0.0000000 \n",
      "\t659/1000: loss: 0.0000000 \n",
      "\t660/1000: loss: 0.0000000 \n",
      "\t661/1000: loss: 0.0000000 \n",
      "\t662/1000: loss: 0.0000000 \n",
      "\t663/1000: loss: 0.0000000 \n",
      "\t664/1000: loss: 0.0000000 \n",
      "\t665/1000: loss: 0.0000000 \n",
      "\t666/1000: loss: 0.0000000 \n",
      "\t667/1000: loss: 0.0000000 \n",
      "\t668/1000: loss: 0.0000000 \n",
      "\t669/1000: loss: 0.0000000 \n",
      "\t670/1000: loss: 0.0000000 \n",
      "\t671/1000: loss: 0.0000000 \n",
      "\t672/1000: loss: 0.0000000 \n",
      "\t673/1000: loss: 0.0000000 \n",
      "\t674/1000: loss: 0.0000000 \n",
      "\t675/1000: loss: 0.0000000 \n",
      "\t676/1000: loss: 0.0000000 \n",
      "\t677/1000: loss: 0.0000000 \n",
      "\t678/1000: loss: 0.0000000 \n",
      "\t679/1000: loss: 0.0000000 \n",
      "\t680/1000: loss: 0.0000000 \n",
      "\t681/1000: loss: 0.0000000 \n",
      "\t682/1000: loss: 0.0000000 \n",
      "\t683/1000: loss: 0.0000000 \n",
      "\t684/1000: loss: 0.0000000 \n",
      "\t685/1000: loss: 0.0000000 \n",
      "\t686/1000: loss: 0.0000000 \n",
      "\t687/1000: loss: 0.0000000 \n",
      "\t688/1000: loss: 0.0000000 \n",
      "\t689/1000: loss: 0.0000000 \n",
      "\t690/1000: loss: 0.0000000 \n",
      "\t691/1000: loss: 0.0000000 \n",
      "\t692/1000: loss: 0.0000000 \n",
      "\t693/1000: loss: 0.0000000 \n",
      "\t694/1000: loss: 0.0000000 \n",
      "\t695/1000: loss: 0.0000000 \n",
      "\t696/1000: loss: 0.0000000 \n",
      "\t697/1000: loss: 0.0000000 \n",
      "\t698/1000: loss: 0.0000000 \n",
      "\t699/1000: loss: 0.0000000 \n",
      "\t700/1000: loss: 0.0000000 \n",
      "\t701/1000: loss: 0.0000000 \n",
      "\t702/1000: loss: 0.0000000 \n",
      "\t703/1000: loss: 0.0000000 \n",
      "\t704/1000: loss: 0.0000000 \n",
      "\t705/1000: loss: 0.0000000 \n",
      "\t706/1000: loss: 0.0000000 \n",
      "\t707/1000: loss: 0.0000000 \n",
      "\t708/1000: loss: 0.0000000 \n",
      "\t709/1000: loss: 0.0000000 \n",
      "\t710/1000: loss: 0.0000000 \n",
      "\t711/1000: loss: 0.0000000 \n",
      "\t712/1000: loss: 0.0000000 \n",
      "\t713/1000: loss: 0.0000000 \n",
      "\t714/1000: loss: 0.0000000 \n",
      "\t715/1000: loss: 0.0000000 \n",
      "\t716/1000: loss: 0.0000000 \n",
      "\t717/1000: loss: 0.0000000 \n",
      "\t718/1000: loss: 0.0000000 \n",
      "\t719/1000: loss: 0.0000000 \n",
      "\t720/1000: loss: 0.0000000 \n",
      "\t721/1000: loss: 0.0000000 \n",
      "\t722/1000: loss: 0.0000000 \n",
      "\t723/1000: loss: 0.0000000 \n",
      "\t724/1000: loss: 0.0000000 \n",
      "\t725/1000: loss: 0.0000000 \n",
      "\t726/1000: loss: 0.0000000 \n",
      "\t727/1000: loss: 0.0000000 \n",
      "\t728/1000: loss: 0.0000000 \n",
      "\t729/1000: loss: 0.0000000 \n",
      "\t730/1000: loss: 0.0000000 \n",
      "\t731/1000: loss: 0.0000000 \n",
      "\t732/1000: loss: 0.0000000 \n",
      "\t733/1000: loss: 0.0000000 \n",
      "\t734/1000: loss: 0.0000000 \n",
      "\t735/1000: loss: 0.0000000 \n",
      "\t736/1000: loss: 0.0000000 \n",
      "\t737/1000: loss: 0.0000000 \n",
      "\t738/1000: loss: 0.0000000 \n",
      "\t739/1000: loss: 0.0000000 \n",
      "\t740/1000: loss: 0.0000000 \n",
      "\t741/1000: loss: 0.0000000 \n",
      "\t742/1000: loss: 0.0000000 \n",
      "\t743/1000: loss: 0.0000000 \n",
      "\t744/1000: loss: 0.0000000 \n",
      "\t745/1000: loss: 0.0000000 \n",
      "\t746/1000: loss: 0.0000000 \n",
      "\t747/1000: loss: 0.0000000 \n",
      "\t748/1000: loss: 0.0000000 \n",
      "\t749/1000: loss: 0.0000000 \n",
      "\t750/1000: loss: 0.0000000 \n",
      "\t751/1000: loss: 0.0000000 \n",
      "\t752/1000: loss: 0.0000000 \n",
      "\t753/1000: loss: 0.0000000 \n",
      "\t754/1000: loss: 0.0000000 \n",
      "\t755/1000: loss: 0.0000000 \n",
      "\t756/1000: loss: 0.0000000 \n",
      "\t757/1000: loss: 0.0000000 \n",
      "\t758/1000: loss: 0.0000000 \n",
      "\t759/1000: loss: 0.0000000 \n",
      "\t760/1000: loss: 0.0000000 \n",
      "\t761/1000: loss: 0.0000000 \n",
      "\t762/1000: loss: 0.0000000 \n",
      "\t763/1000: loss: 0.0000000 \n",
      "\t764/1000: loss: 0.0000000 \n",
      "\t765/1000: loss: 0.0000000 \n",
      "\t766/1000: loss: 0.0000000 \n",
      "\t767/1000: loss: 0.0000000 \n",
      "\t768/1000: loss: 0.0000000 \n",
      "\t769/1000: loss: 0.0000000 \n",
      "\t770/1000: loss: 0.0000000 \n",
      "\t771/1000: loss: 0.0000000 \n",
      "\t772/1000: loss: 0.0000000 \n",
      "\t773/1000: loss: 0.0000000 \n",
      "\t774/1000: loss: 0.0000000 \n",
      "\t775/1000: loss: 0.0000000 \n",
      "\t776/1000: loss: 0.0000000 \n",
      "\t777/1000: loss: 0.0000000 \n",
      "\t778/1000: loss: 0.0000000 \n",
      "\t779/1000: loss: 0.0000000 \n",
      "\t780/1000: loss: 0.0000000 \n",
      "\t781/1000: loss: 0.0000000 \n",
      "\t782/1000: loss: 0.0000000 \n",
      "\t783/1000: loss: 0.0000000 \n",
      "\t784/1000: loss: 0.0000000 \n",
      "\t785/1000: loss: 0.0000000 \n",
      "\t786/1000: loss: 0.0000000 \n",
      "\t787/1000: loss: 0.0000000 \n",
      "\t788/1000: loss: 0.0000000 \n",
      "\t789/1000: loss: 0.0000000 \n",
      "\t790/1000: loss: 0.0000000 \n",
      "\t791/1000: loss: 0.0000000 \n",
      "\t792/1000: loss: 0.0000000 \n",
      "\t793/1000: loss: 0.0000000 \n",
      "\t794/1000: loss: 0.0000000 \n",
      "\t795/1000: loss: 0.0000000 \n",
      "\t796/1000: loss: 0.0000000 \n",
      "\t797/1000: loss: 0.0000000 \n",
      "\t798/1000: loss: 0.0000000 \n",
      "\t799/1000: loss: 0.0000000 \n",
      "\t800/1000: loss: 0.0000000 \n",
      "\t801/1000: loss: 0.0000000 \n",
      "\t802/1000: loss: 0.0000000 \n",
      "\t803/1000: loss: 0.0000000 \n",
      "\t804/1000: loss: 0.0000000 \n",
      "\t805/1000: loss: 0.0000000 \n",
      "\t806/1000: loss: 0.0000000 \n",
      "\t807/1000: loss: 0.0000000 \n",
      "\t808/1000: loss: 0.0000000 \n",
      "\t809/1000: loss: 0.0000000 \n",
      "\t810/1000: loss: 0.0000000 \n",
      "\t811/1000: loss: 0.0000000 \n",
      "\t812/1000: loss: 0.0000000 \n",
      "\t813/1000: loss: 0.0000000 \n",
      "\t814/1000: loss: 0.0000000 \n",
      "\t815/1000: loss: 0.0000000 \n",
      "\t816/1000: loss: 0.0000000 \n",
      "\t817/1000: loss: 0.0000000 \n",
      "\t818/1000: loss: 0.0000000 \n",
      "\t819/1000: loss: 0.0000000 \n",
      "\t820/1000: loss: 0.0000000 \n",
      "\t821/1000: loss: 0.0000000 \n",
      "\t822/1000: loss: 0.0000000 \n",
      "\t823/1000: loss: 0.0000000 \n",
      "\t824/1000: loss: 0.0000000 \n",
      "\t825/1000: loss: 0.0000000 \n",
      "\t826/1000: loss: 0.0000000 \n",
      "\t827/1000: loss: 0.0000000 \n",
      "\t828/1000: loss: 0.0000000 \n",
      "\t829/1000: loss: 0.0000000 \n",
      "\t830/1000: loss: 0.0000000 \n",
      "\t831/1000: loss: 0.0000000 \n",
      "\t832/1000: loss: 0.0000000 \n",
      "\t833/1000: loss: 0.0000000 \n",
      "\t834/1000: loss: 0.0000000 \n",
      "\t835/1000: loss: 0.0000000 \n",
      "\t836/1000: loss: 0.0000000 \n",
      "\t837/1000: loss: 0.0000000 \n",
      "\t838/1000: loss: 0.0000000 \n",
      "\t839/1000: loss: 0.0000000 \n",
      "\t840/1000: loss: 0.0000000 \n",
      "\t841/1000: loss: 0.0000000 \n",
      "\t842/1000: loss: 0.0000000 \n",
      "\t843/1000: loss: 0.0000000 \n",
      "\t844/1000: loss: 0.0000000 \n",
      "\t845/1000: loss: 0.0000000 \n",
      "\t846/1000: loss: 0.0000000 \n",
      "\t847/1000: loss: 0.0000000 \n",
      "\t848/1000: loss: 0.0000000 \n",
      "\t849/1000: loss: 0.0000000 \n",
      "\t850/1000: loss: 0.0000000 \n",
      "\t851/1000: loss: 0.0000000 \n",
      "\t852/1000: loss: 0.0000000 \n",
      "\t853/1000: loss: 0.0000000 \n",
      "\t854/1000: loss: 0.0000000 \n",
      "\t855/1000: loss: 0.0000000 \n",
      "\t856/1000: loss: 0.0000000 \n",
      "\t857/1000: loss: 0.0000000 \n",
      "\t858/1000: loss: 0.0000000 \n",
      "\t859/1000: loss: 0.0000000 \n",
      "\t860/1000: loss: 0.0000000 \n",
      "\t861/1000: loss: 0.0000000 \n",
      "\t862/1000: loss: 0.0000000 \n",
      "\t863/1000: loss: 0.0000000 \n",
      "\t864/1000: loss: 0.0000000 \n",
      "\t865/1000: loss: 0.0000000 \n",
      "\t866/1000: loss: 0.0000000 \n",
      "\t867/1000: loss: 0.0000000 \n",
      "\t868/1000: loss: 0.0000000 \n",
      "\t869/1000: loss: 0.0000000 \n",
      "\t870/1000: loss: 0.0000000 \n",
      "\t871/1000: loss: 0.0000000 \n",
      "\t872/1000: loss: 0.0000000 \n",
      "\t873/1000: loss: 0.0000000 \n",
      "\t874/1000: loss: 0.0000000 \n",
      "\t875/1000: loss: 0.0000000 \n",
      "\t876/1000: loss: 0.0000000 \n",
      "\t877/1000: loss: 0.0000000 \n",
      "\t878/1000: loss: 0.0000000 \n",
      "\t879/1000: loss: 0.0000000 \n",
      "\t880/1000: loss: 0.0000000 \n",
      "\t881/1000: loss: 0.0000000 \n",
      "\t882/1000: loss: 0.0000000 \n",
      "\t883/1000: loss: 0.0000000 \n",
      "\t884/1000: loss: 0.0000000 \n",
      "\t885/1000: loss: 0.0000000 \n",
      "\t886/1000: loss: 0.0000000 \n",
      "\t887/1000: loss: 0.0000000 \n",
      "\t888/1000: loss: 0.0000000 \n",
      "\t889/1000: loss: 0.0000000 \n",
      "\t890/1000: loss: 0.0000000 \n",
      "\t891/1000: loss: 0.0000000 \n",
      "\t892/1000: loss: 0.0000000 \n",
      "\t893/1000: loss: 0.0000000 \n",
      "\t894/1000: loss: 0.0000000 \n",
      "\t895/1000: loss: 0.0000000 \n",
      "\t896/1000: loss: 0.0000000 \n",
      "\t897/1000: loss: 0.0000000 \n",
      "\t898/1000: loss: 0.0000000 \n",
      "\t899/1000: loss: 0.0000000 \n",
      "\t900/1000: loss: 0.0000000 \n",
      "\t901/1000: loss: 0.0000000 \n",
      "\t902/1000: loss: 0.0000000 \n",
      "\t903/1000: loss: 0.0000000 \n",
      "\t904/1000: loss: 0.0000000 \n",
      "\t905/1000: loss: 0.0000000 \n",
      "\t906/1000: loss: 0.0000000 \n",
      "\t907/1000: loss: 0.0000000 \n",
      "\t908/1000: loss: 0.0000000 \n",
      "\t909/1000: loss: 0.0000000 \n",
      "\t910/1000: loss: 0.0000000 \n",
      "\t911/1000: loss: 0.0000000 \n",
      "\t912/1000: loss: 0.0000000 \n",
      "\t913/1000: loss: 0.0000000 \n",
      "\t914/1000: loss: 0.0000000 \n",
      "\t915/1000: loss: 0.0000000 \n",
      "\t916/1000: loss: 0.0000000 \n",
      "\t917/1000: loss: 0.0000000 \n",
      "\t918/1000: loss: 0.0000000 \n",
      "\t919/1000: loss: 0.0000000 \n",
      "\t920/1000: loss: 0.0000000 \n",
      "\t921/1000: loss: 0.0000000 \n",
      "\t922/1000: loss: 0.0000000 \n",
      "\t923/1000: loss: 0.0000000 \n",
      "\t924/1000: loss: 0.0000000 \n",
      "\t925/1000: loss: 0.0000000 \n",
      "\t926/1000: loss: 0.0000000 \n",
      "\t927/1000: loss: 0.0000000 \n",
      "\t928/1000: loss: 0.0000000 \n",
      "\t929/1000: loss: 0.0000000 \n",
      "\t930/1000: loss: 0.0000000 \n",
      "\t931/1000: loss: 0.0000000 \n",
      "\t932/1000: loss: 0.0000000 \n",
      "\t933/1000: loss: 0.0000000 \n",
      "\t934/1000: loss: 0.0000000 \n",
      "\t935/1000: loss: 0.0000000 \n",
      "\t936/1000: loss: 0.0000000 \n",
      "\t937/1000: loss: 0.0000000 \n",
      "\t938/1000: loss: 0.0000000 \n",
      "\t939/1000: loss: 0.0000000 \n",
      "\t940/1000: loss: 0.0000000 \n",
      "\t941/1000: loss: 0.0000000 \n",
      "\t942/1000: loss: 0.0000000 \n",
      "\t943/1000: loss: 0.0000000 \n",
      "\t944/1000: loss: 0.0000000 \n",
      "\t945/1000: loss: 0.0000000 \n",
      "\t946/1000: loss: 0.0000000 \n",
      "\t947/1000: loss: 0.0000000 \n",
      "\t948/1000: loss: 0.0000000 \n",
      "\t949/1000: loss: 0.0000000 \n",
      "\t950/1000: loss: 0.0000000 \n",
      "\t951/1000: loss: 0.0000000 \n",
      "\t952/1000: loss: 0.0000000 \n",
      "\t953/1000: loss: 0.0000000 \n",
      "\t954/1000: loss: 0.0000000 \n",
      "\t955/1000: loss: 0.0000000 \n",
      "\t956/1000: loss: 0.0000000 \n",
      "\t957/1000: loss: 0.0000000 \n",
      "\t958/1000: loss: 0.0000000 \n",
      "\t959/1000: loss: 0.0000000 \n",
      "\t960/1000: loss: 0.0000000 \n",
      "\t961/1000: loss: 0.0000000 \n",
      "\t962/1000: loss: 0.0000000 \n",
      "\t963/1000: loss: 0.0000000 \n",
      "\t964/1000: loss: 0.0000000 \n",
      "\t965/1000: loss: 0.0000000 \n",
      "\t966/1000: loss: 0.0000000 \n",
      "\t967/1000: loss: 0.0000000 \n",
      "\t968/1000: loss: 0.0000000 \n",
      "\t969/1000: loss: 0.0000000 \n",
      "\t970/1000: loss: 0.0000000 \n",
      "\t971/1000: loss: 0.0000000 \n",
      "\t972/1000: loss: 0.0000000 \n",
      "\t973/1000: loss: 0.0000000 \n",
      "\t974/1000: loss: 0.0000000 \n",
      "\t975/1000: loss: 0.0000000 \n",
      "\t976/1000: loss: 0.0000000 \n",
      "\t977/1000: loss: 0.0000000 \n",
      "\t978/1000: loss: 0.0000000 \n",
      "\t979/1000: loss: 0.0000000 \n",
      "\t980/1000: loss: 0.0000000 \n",
      "\t981/1000: loss: 0.0000000 \n",
      "\t982/1000: loss: 0.0000000 \n",
      "\t983/1000: loss: 0.0000000 \n",
      "\t984/1000: loss: 0.0000000 \n",
      "\t985/1000: loss: 0.0000000 \n",
      "\t986/1000: loss: 0.0000000 \n",
      "\t987/1000: loss: 0.0000000 \n",
      "\t988/1000: loss: 0.0000000 \n",
      "\t989/1000: loss: 0.0000000 \n",
      "\t990/1000: loss: 0.0000000 \n",
      "\t991/1000: loss: 0.0000000 \n",
      "\t992/1000: loss: 0.0000000 \n",
      "\t993/1000: loss: 0.0000000 \n",
      "\t994/1000: loss: 0.0000000 \n",
      "\t995/1000: loss: 0.0000000 \n",
      "\t996/1000: loss: 0.0000000 \n",
      "\t997/1000: loss: 0.0000000 \n",
      "\t998/1000: loss: 0.0000000 \n",
      "\t999/1000: loss: 0.0000000 \n",
      "\t1000/1000: loss: 0.0000000 \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(1000):\n",
    "    out = nac(X_train)\n",
    "    loss = crieterion(out, y_train)\n",
    "    losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\t{}/{}: loss: {:.7f} \".format(i+1, num_iters, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:10])\n",
    "plt.title('Losses in 10 epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 9.989236000873536e-13\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_out = nac(X_train)\n",
    "    train_loss = crieterion(train_out, y_train)\n",
    "    print(f'train loss = {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss = 9.74864611778814e-13\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_out = nac(X_test)\n",
    "    test_loss = crieterion(test_out, y_test)\n",
    "    print(f'test loss = {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Accumulator vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4536])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1/100000: mlp loss: 172.5703430 \n",
      "\t1001/100000: mlp loss: 172.5703430 \n",
      "\t2001/100000: mlp loss: 172.5703430 \n",
      "\t3001/100000: mlp loss: 172.5703430 \n",
      "\t4001/100000: mlp loss: 172.5703430 \n",
      "\t5001/100000: mlp loss: 172.5703430 \n",
      "\t6001/100000: mlp loss: 172.5703430 \n",
      "\t7001/100000: mlp loss: 172.5703430 \n",
      "\t8001/100000: mlp loss: 172.5703430 \n",
      "\t9001/100000: mlp loss: 172.5703430 \n",
      "\t10001/100000: mlp loss: 172.5703430 \n",
      "\t11001/100000: mlp loss: 172.5703430 \n",
      "\t12001/100000: mlp loss: 172.5703430 \n",
      "\t13001/100000: mlp loss: 172.5703430 \n",
      "\t14001/100000: mlp loss: 172.5703430 \n",
      "\t15001/100000: mlp loss: 172.5703430 \n",
      "\t16001/100000: mlp loss: 172.5703430 \n",
      "\t17001/100000: mlp loss: 172.5703430 \n",
      "\t18001/100000: mlp loss: 172.5703430 \n",
      "\t19001/100000: mlp loss: 172.5703430 \n",
      "\t20001/100000: mlp loss: 172.5703430 \n",
      "\t21001/100000: mlp loss: 172.5703430 \n",
      "\t22001/100000: mlp loss: 172.5703430 \n",
      "\t23001/100000: mlp loss: 172.5703430 \n",
      "\t24001/100000: mlp loss: 172.5703430 \n",
      "\t25001/100000: mlp loss: 172.5703430 \n",
      "\t26001/100000: mlp loss: 172.5703430 \n",
      "\t27001/100000: mlp loss: 172.5703430 \n",
      "\t28001/100000: mlp loss: 172.5703430 \n",
      "\t29001/100000: mlp loss: 172.5703430 \n",
      "\t30001/100000: mlp loss: 172.5703430 \n",
      "\t31001/100000: mlp loss: 172.5703430 \n",
      "\t32001/100000: mlp loss: 172.5703430 \n",
      "\t33001/100000: mlp loss: 172.5703430 \n",
      "\t34001/100000: mlp loss: 172.5703430 \n",
      "\t35001/100000: mlp loss: 172.5703430 \n",
      "\t36001/100000: mlp loss: 172.5703430 \n",
      "\t37001/100000: mlp loss: 172.5703430 \n",
      "\t38001/100000: mlp loss: 172.5703430 \n",
      "\t39001/100000: mlp loss: 172.5703430 \n",
      "\t40001/100000: mlp loss: 172.5703430 \n",
      "\t41001/100000: mlp loss: 172.5703430 \n",
      "\t42001/100000: mlp loss: 172.5703430 \n",
      "\t43001/100000: mlp loss: 172.5703430 \n",
      "\t44001/100000: mlp loss: 172.5703430 \n",
      "\t45001/100000: mlp loss: 172.5703430 \n",
      "\t46001/100000: mlp loss: 172.5703430 \n",
      "\t47001/100000: mlp loss: 172.5703430 \n",
      "\t48001/100000: mlp loss: 172.5703430 \n",
      "\t49001/100000: mlp loss: 172.5703430 \n",
      "\t50001/100000: mlp loss: 172.5703430 \n",
      "\t51001/100000: mlp loss: 172.5703430 \n",
      "\t52001/100000: mlp loss: 172.5703430 \n",
      "\t53001/100000: mlp loss: 172.5703430 \n",
      "\t54001/100000: mlp loss: 172.5703430 \n",
      "\t55001/100000: mlp loss: 172.5703430 \n",
      "\t56001/100000: mlp loss: 172.5703430 \n",
      "\t57001/100000: mlp loss: 172.5703430 \n",
      "\t58001/100000: mlp loss: 172.5703430 \n",
      "\t59001/100000: mlp loss: 172.5703430 \n",
      "\t60001/100000: mlp loss: 172.5703430 \n",
      "\t61001/100000: mlp loss: 172.5703430 \n",
      "\t62001/100000: mlp loss: 172.5703430 \n",
      "\t63001/100000: mlp loss: 172.5703430 \n",
      "\t64001/100000: mlp loss: 172.5703430 \n",
      "\t65001/100000: mlp loss: 172.5703430 \n",
      "\t66001/100000: mlp loss: 172.5703430 \n",
      "\t67001/100000: mlp loss: 172.5703430 \n",
      "\t68001/100000: mlp loss: 172.5703430 \n",
      "\t69001/100000: mlp loss: 172.5703430 \n",
      "\t70001/100000: mlp loss: 172.5703430 \n",
      "\t71001/100000: mlp loss: 172.5703430 \n",
      "\t72001/100000: mlp loss: 172.5703430 \n",
      "\t73001/100000: mlp loss: 172.5703430 \n",
      "\t74001/100000: mlp loss: 172.5703430 \n",
      "\t75001/100000: mlp loss: 172.5703430 \n",
      "\t76001/100000: mlp loss: 172.5703430 \n",
      "\t77001/100000: mlp loss: 172.5703430 \n",
      "\t78001/100000: mlp loss: 172.5703430 \n",
      "\t79001/100000: mlp loss: 172.5703430 \n",
      "\t80001/100000: mlp loss: 172.5703430 \n",
      "\t81001/100000: mlp loss: 172.5703430 \n",
      "\t82001/100000: mlp loss: 172.5703430 \n",
      "\t83001/100000: mlp loss: 172.5703430 \n",
      "\t84001/100000: mlp loss: 172.5703430 \n",
      "\t85001/100000: mlp loss: 172.5703430 \n",
      "\t86001/100000: mlp loss: 172.5703430 \n",
      "\t87001/100000: mlp loss: 172.5703430 \n",
      "\t88001/100000: mlp loss: 172.5703430 \n",
      "\t89001/100000: mlp loss: 172.5703430 \n",
      "\t90001/100000: mlp loss: 172.5703430 \n",
      "\t91001/100000: mlp loss: 172.5703430 \n",
      "\t92001/100000: mlp loss: 172.5703430 \n",
      "\t93001/100000: mlp loss: 172.5703430 \n",
      "\t94001/100000: mlp loss: 172.5703430 \n",
      "\t95001/100000: mlp loss: 172.5703430 \n",
      "\t96001/100000: mlp loss: 172.5703430 \n",
      "\t97001/100000: mlp loss: 172.5703430 \n",
      "\t98001/100000: mlp loss: 172.5703430 \n",
      "\t99001/100000: mlp loss: 172.5703430 \n"
     ]
    }
   ],
   "source": [
    "mlp_losses = []\n",
    "num_iters = 100000\n",
    "for i in range(num_iters):\n",
    "    out = mlp(X_train)\n",
    "    loss = crieterion(out, y_train)\n",
    "    mlp_losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i% 1000 == 0:\n",
    "        print(\"\\t{}/{}: mlp loss: {:.7f} \".format(i+1, num_iters, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp_losses)\n",
    "plt.title('MLP Losses in 10000 epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Arithmetic Logic Unit Cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALU(nn.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (dlenv)",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
